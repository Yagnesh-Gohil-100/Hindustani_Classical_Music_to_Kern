{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroup Range: (4, 7)\n",
      "Swar List: [[(0, 5, 12, 406, 162, 10, 12)], [(0, 5, 13, 439, 162, 10, 14)], [(0, 5, 14, 472, 166, 9, 5)]]\n",
      "Kann Swar List: [[(0, 4, 12, 400, 145, 7, 10)], [(0, 4, 13, 434, 145, 9, 10)], []]\n",
      "----------------------------------------\n",
      "Subgroup Range: (7, 8)\n",
      "Swar List: [[(0, 7, 1, 118, 231, 9, 13)], [(0, 7, 2, 139, 236, 9, 4)], [(0, 7, 3, 156, 211, 14, 12), (0, 7, 3, 159, 227, 14, 16)], [(0, 7, 4, 181, 214, 17, 29)], [(0, 7, 5, 215, 227, 15, 16)], [(0, 7, 6, 240, 235, 10, 5)], [(0, 7, 7, 260, 235, 11, 5)], [(0, 7, 8, 281, 226, 14, 17)], [(0, 7, 9, 303, 213, 8, 8), (0, 7, 9, 304, 226, 14, 16)], [(0, 7, 10, 339, 227, 15, 15)], [(0, 7, 11, 376, 235, 9, 4)], [(0, 7, 12, 401, 211, 13, 10), (0, 7, 12, 405, 225, 7, 17)], [(0, 7, 13, 434, 226, 15, 15)], [(0, 7, 14, 470, 234, 10, 4)]]\n",
      "Kann Swar List: [[], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "----------------------------------------\n",
      "Subgroup Range: (10, 14)\n",
      "Swar List: [[(0, 11, 1, 117, 314, 10, 12)], [(0, 11, 2, 137, 309, 22, 23)], [(0, 11, 3, 169, 313, 8, 13)], [(0, 11, 4, 189, 318, 9, 4)], [(0, 11, 5, 215, 312, 10, 13)], [(0, 11, 6, 235, 313, 10, 12)], [(0, 11, 7, 255, 307, 15, 14)], [(0, 11, 8, 285, 312, 10, 13)], [(0, 11, 9, 309, 312, 9, 12)], [(0, 11, 10, 338, 312, 9, 12)], [(0, 11, 11, 367, 311, 18, 9)], [(0, 11, 12, 405, 312, 10, 13)], [(0, 11, 13, 437, 305, 8, 19)], [(0, 11, 14, 471, 316, 10, 4)]]\n",
      "Kann Swar List: [[(0, 10, 1, 116, 294, 14, 13)], [], [], [], [(0, 10, 5, 213, 296, 6, 9)], [(0, 10, 6, 232, 296, 7, 9)], [], [(0, 10, 8, 278, 295, 9, 9)], [], [], [], [], [], []]\n",
      "----------------------------------------\n",
      "Subgroup Range: (16, 19)\n",
      "Swar List: [[(0, 18, 1, 116, 395, 15, 12)], [(0, 18, 2, 142, 400, 10, 4)], [(0, 18, 3, 162, 400, 11, 4)], [(0, 18, 4, 183, 395, 15, 12)], [(0, 18, 5, 215, 392, 14, 15)], [(0, 18, 7, 239, 391, 15, 16)], [(0, 18, 8, 263, 399, 10, 5)], [(0, 18, 10, 284, 395, 10, 12)], [(0, 18, 11, 308, 399, 9, 5)], [(0, 18, 12, 338, 391, 13, 19)], [(0, 18, 13, 372, 394, 9, 13)], [(0, 18, 14, 404, 394, 10, 13)], [(0, 18, 15, 437, 394, 10, 13)], [(0, 18, 17, 468, 399, 12, 4)]]\n",
      "Kann Swar List: [[], [], [], [], [(0, 16, 5, 213, 374, 12, 14)], [], [], [], [], [], [], [(0, 16, 14, 399, 377, 6, 10)], [(0, 16, 15, 438, 377, 9, 10)], []]\n",
      "----------------------------------------\n",
      "Subgroup Range: (22, 24)\n",
      "Swar List: [[(0, 23, 1, 115, 495, 14, 15)], [(0, 23, 2, 137, 502, 10, 5)], [(0, 23, 4, 158, 502, 10, 5)], [(0, 23, 6, 179, 494, 14, 16)], [(0, 23, 8, 204, 493, 14, 17)], [(0, 23, 9, 239, 494, 15, 16)], [(0, 23, 10, 275, 502, 9, 5)], [(0, 23, 11, 306, 493, 15, 17)], [(0, 23, 12, 336, 503, 9, 4)], [(0, 23, 13, 364, 494, 15, 16)], [(0, 23, 14, 403, 494, 15, 16)], [(0, 23, 15, 424, 493, 8, 18)], [(0, 23, 16, 442, 493, 15, 17)], [(0, 23, 17, 467, 502, 12, 4)]]\n",
      "Kann Swar List: [[(0, 22, 1, 114, 477, 12, 14)], [], [], [(0, 22, 6, 173, 478, 13, 12)], [], [], [], [], [], [], [(0, 22, 14, 397, 477, 12, 13)], [], [], []]\n",
      "----------------------------------------\n",
      "Subgroup Range: (26, 27)\n",
      "Swar List: [[(0, 26, 1, 113, 560, 13, 13), (0, 26, 1, 114, 577, 15, 16)], [(0, 26, 2, 140, 576, 10, 17)], [(0, 26, 3, 161, 585, 9, 4)], [(0, 26, 4, 179, 558, 8, 16)], [(0, 26, 5, 185, 576, 10, 18)], [(0, 26, 6, 208, 585, 10, 4)], [(0, 26, 7, 238, 585, 10, 5)], [(0, 26, 8, 274, 577, 10, 16)], [(0, 26, 9, 300, 561, 12, 31)], [(0, 26, 10, 337, 585, 10, 4)], [(0, 26, 11, 368, 585, 12, 4)], [(0, 26, 12, 397, 560, 12, 13), (0, 26, 12, 402, 576, 15, 17)], [(0, 26, 13, 424, 575, 7, 18)], [(0, 26, 14, 441, 575, 15, 18)], [(0, 26, 15, 463, 576, 15, 17)]]\n",
      "Kann Swar List: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "----------------------------------------\n",
      "Subgroup Range: (29, 33)\n",
      "Swar List: [[(0, 30, 1, 113, 663, 11, 12)], [(0, 30, 2, 133, 658, 21, 12)], [(0, 30, 3, 163, 657, 8, 19)], [(0, 30, 4, 181, 658, 15, 17)], [(0, 30, 5, 203, 658, 15, 17)], [(0, 30, 6, 239, 663, 10, 12)], [(0, 30, 7, 265, 658, 19, 14)], [(0, 30, 8, 303, 662, 10, 13)], [(0, 30, 10, 336, 662, 11, 14)], [(0, 30, 11, 370, 667, 10, 4)]]\n",
      "Kann Swar List: [[(0, 29, 1, 112, 645, 10, 10)], [], [], [], [], [(0, 29, 6, 233, 645, 10, 9)], [], [(0, 29, 8, 298, 646, 7, 10)], [(0, 29, 10, 332, 646, 9, 10)], []]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the path to the folder containing the images\n",
    "image_folder_path = 'Analysis/bilawal_dhamaar'\n",
    "\n",
    "# Define the section-wise row numbers\n",
    "articulation_rows = [12, 14, 31, 33]\n",
    "kann_swar_rows = [4, 10, 16, 22, 29]\n",
    "swar_rows = [5, 7, 11, 18, 23, 26, 30]\n",
    "lyrics_rows = [6, 8, 13, 19, 24, 27, 32]\n",
    "\n",
    "# Define the subgroup ranges\n",
    "subgroup_ranges = [\n",
    "    (4, 7),\n",
    "    (7, 8),\n",
    "    (10, 14),\n",
    "    (16, 19),\n",
    "    (22, 24),\n",
    "    (26, 27),\n",
    "    (29, 33)\n",
    "]\n",
    "\n",
    "# Function to extract information from the image filename\n",
    "def extract_info_from_filename(filename):\n",
    "    pattern = r'(\\d+)_row(\\d+)(?:_col(\\d+))?_x(\\d+)_y(\\d+)_w(\\d+)_h(\\d+)'\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        page_num = int(match.group(1))\n",
    "        row_num = int(match.group(2))\n",
    "        col_num = int(match.group(3)) if match.group(3) else None\n",
    "        x = int(match.group(4))\n",
    "        y = int(match.group(5))\n",
    "        width = int(match.group(6))\n",
    "        height = int(match.group(7))\n",
    "        return page_num, row_num, col_num, x, y, width, height\n",
    "    return None\n",
    "\n",
    "# Load all image filenames and extract their information\n",
    "image_files = os.listdir(image_folder_path)\n",
    "image_info = [extract_info_from_filename(f) for f in image_files]\n",
    "image_info = [info for info in image_info if info is not None]\n",
    "\n",
    "# Organize images by row and column\n",
    "row_col_images = defaultdict(lambda: defaultdict(list))\n",
    "for info in image_info:\n",
    "    page_num, row_num, col_num, x, y, width, height = info\n",
    "    row_col_images[row_num][col_num].append(info)\n",
    "\n",
    "# Function to process a subgroup and create the lists of lists\n",
    "def process_subgroup(subgroup_range):\n",
    "    start_row, end_row = subgroup_range\n",
    "    \n",
    "    # Find the swar row in this subgroup\n",
    "    swar_row = None\n",
    "    for row in swar_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            swar_row = row\n",
    "            break\n",
    "    \n",
    "    if not swar_row:\n",
    "        return None, None\n",
    "    \n",
    "    # Find the kann swar row in this subgroup\n",
    "    kann_swar_row = None\n",
    "    for row in kann_swar_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            kann_swar_row = row\n",
    "            break\n",
    "    \n",
    "    # Find the articulation and lyrics rows (if any)\n",
    "    articulation_row = None\n",
    "    for row in articulation_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            articulation_row = row\n",
    "            break\n",
    "    \n",
    "    lyrics_row = None\n",
    "    for row in lyrics_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            lyrics_row = row\n",
    "            break\n",
    "    \n",
    "    # Get the swar images and their column numbers\n",
    "    swar_images = row_col_images[swar_row]\n",
    "    swar_cols = sorted(swar_images.keys())\n",
    "    \n",
    "    # Get the kann swar images and their column numbers\n",
    "    kann_swar_images = row_col_images[kann_swar_row] if kann_swar_row else {}\n",
    "    kann_swar_cols = sorted(kann_swar_images.keys())\n",
    "    \n",
    "    # Create the lists of lists\n",
    "    swar_list = []\n",
    "    kann_swar_list = []\n",
    "    \n",
    "    for col in swar_cols:\n",
    "        swar_list.append(swar_images[col])\n",
    "        if col in kann_swar_cols:\n",
    "            kann_swar_list.append(kann_swar_images[col])\n",
    "        else:\n",
    "            kann_swar_list.append([])\n",
    "    \n",
    "    return swar_list, kann_swar_list\n",
    "\n",
    "# Process each subgroup and store the results\n",
    "subgroup_results = {}\n",
    "for subgroup_range in subgroup_ranges:\n",
    "    swar_list, kann_swar_list = process_subgroup(subgroup_range)\n",
    "    if swar_list and kann_swar_list:\n",
    "        subgroup_results[subgroup_range] = {\n",
    "            'swar_list': swar_list,\n",
    "            'kann_swar_list': kann_swar_list\n",
    "        }\n",
    "\n",
    "# Print the results for each subgroup\n",
    "for subgroup_range, results in subgroup_results.items():\n",
    "    print(f\"Subgroup Range: {subgroup_range}\")\n",
    "    print(f\"Swar List: {results['swar_list']}\")\n",
    "    print(f\"Kann Swar List: {results['kann_swar_list']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# working almost fine when subgroup has one swar row and one kann swar row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroup Range: (4, 7)\n",
      "Swar List: [[(0, 5, 12, 406, 162, 10, 12)], [(0, 5, 13, 439, 162, 10, 14)], [(0, 5, 14, 472, 166, 9, 5)]]\n",
      "Kann Swar List: [[(0, 4, 12, 400, 145, 7, 10)], [(0, 4, 13, 434, 145, 9, 10)], []]\n",
      "----------------------------------------\n",
      "Subgroup Range: (7, 8)\n",
      "Swar List: [[(0, 7, 1, 118, 231, 9, 13)], [(0, 7, 2, 139, 236, 9, 4)], [(0, 7, 3, 156, 211, 14, 12), (0, 7, 3, 159, 227, 14, 16)], [(0, 7, 4, 181, 214, 17, 29)], [(0, 7, 5, 215, 227, 15, 16)], [(0, 7, 6, 240, 235, 10, 5)], [(0, 7, 7, 260, 235, 11, 5)], [(0, 7, 8, 281, 226, 14, 17)], [(0, 7, 9, 303, 213, 8, 8), (0, 7, 9, 304, 226, 14, 16)], [(0, 7, 10, 339, 227, 15, 15)], [(0, 7, 11, 376, 235, 9, 4)], [(0, 7, 12, 401, 211, 13, 10), (0, 7, 12, 405, 225, 7, 17)], [(0, 7, 13, 434, 226, 15, 15)], [(0, 7, 14, 470, 234, 10, 4)]]\n",
      "Kann Swar List: [[], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "----------------------------------------\n",
      "Subgroup Range: (10, 14)\n",
      "Swar List: [[(0, 11, 1, 117, 314, 10, 12)], [(0, 11, 2, 137, 309, 22, 23)], [(0, 11, 3, 169, 313, 8, 13)], [(0, 11, 4, 189, 318, 9, 4)], [(0, 11, 5, 215, 312, 10, 13)], [(0, 11, 6, 235, 313, 10, 12)], [(0, 11, 7, 255, 307, 15, 14)], [(0, 11, 8, 285, 312, 10, 13)], [(0, 11, 9, 309, 312, 9, 12)], [(0, 11, 10, 338, 312, 9, 12)], [(0, 11, 11, 367, 311, 18, 9)], [(0, 11, 12, 405, 312, 10, 13)], [(0, 11, 13, 437, 305, 8, 19)], [(0, 11, 14, 471, 316, 10, 4)]]\n",
      "Kann Swar List: [[(0, 10, 1, 116, 294, 14, 13)], [], [], [], [(0, 10, 5, 213, 296, 6, 9)], [(0, 10, 6, 232, 296, 7, 9)], [], [(0, 10, 8, 278, 295, 9, 9)], [], [], [], [], [], []]\n",
      "----------------------------------------\n",
      "Subgroup Range: (16, 19)\n",
      "Swar List: [[(0, 18, 1, 116, 395, 15, 12)], [(0, 18, 2, 142, 400, 10, 4)], [(0, 18, 3, 162, 400, 11, 4)], [(0, 18, 4, 183, 395, 15, 12)], [(0, 18, 5, 215, 392, 14, 15)], [(0, 18, 7, 239, 391, 15, 16)], [(0, 18, 8, 263, 399, 10, 5)], [(0, 18, 10, 284, 395, 10, 12)], [(0, 18, 11, 308, 399, 9, 5)], [(0, 18, 12, 338, 391, 13, 19)], [(0, 18, 13, 372, 394, 9, 13)], [(0, 18, 14, 404, 394, 10, 13)], [(0, 18, 15, 437, 394, 10, 13)], [(0, 18, 17, 468, 399, 12, 4)]]\n",
      "Kann Swar List: [[], [], [], [], [(0, 16, 5, 213, 374, 12, 14)], [], [], [(0, 16, 9, 273, 376, 14, 12)], [], [], [], [(0, 16, 14, 399, 377, 6, 10)], [(0, 16, 15, 438, 377, 9, 10)], []]\n",
      "----------------------------------------\n",
      "Subgroup Range: (22, 24)\n",
      "Swar List: [[(0, 23, 1, 115, 495, 14, 15)], [(0, 23, 2, 137, 502, 10, 5)], [(0, 23, 4, 158, 502, 10, 5)], [(0, 23, 6, 179, 494, 14, 16)], [(0, 23, 8, 204, 493, 14, 17)], [(0, 23, 9, 239, 494, 15, 16)], [(0, 23, 10, 275, 502, 9, 5)], [(0, 23, 11, 306, 493, 15, 17)], [(0, 23, 12, 336, 503, 9, 4)], [(0, 23, 13, 364, 494, 15, 16)], [(0, 23, 14, 403, 494, 15, 16)], [(0, 23, 15, 424, 493, 8, 18)], [(0, 23, 16, 442, 493, 15, 17)], [(0, 23, 17, 467, 502, 12, 4)]]\n",
      "Kann Swar List: [[(0, 22, 1, 114, 477, 12, 14)], [], [], [(0, 22, 6, 173, 478, 13, 12)], [(0, 22, 7, 194, 481, 9, 10)], [], [], [], [], [], [(0, 22, 14, 397, 477, 12, 13)], [], [], []]\n",
      "----------------------------------------\n",
      "Subgroup Range: (26, 27)\n",
      "Swar List: [[(0, 26, 1, 113, 560, 13, 13), (0, 26, 1, 114, 577, 15, 16)], [(0, 26, 2, 140, 576, 10, 17)], [(0, 26, 3, 161, 585, 9, 4)], [(0, 26, 4, 179, 558, 8, 16)], [(0, 26, 5, 185, 576, 10, 18)], [(0, 26, 6, 208, 585, 10, 4)], [(0, 26, 7, 238, 585, 10, 5)], [(0, 26, 8, 274, 577, 10, 16)], [(0, 26, 9, 300, 561, 12, 31)], [(0, 26, 10, 337, 585, 10, 4)], [(0, 26, 11, 368, 585, 12, 4)], [(0, 26, 12, 397, 560, 12, 13), (0, 26, 12, 402, 576, 15, 17)], [(0, 26, 13, 424, 575, 7, 18)], [(0, 26, 14, 441, 575, 15, 18)], [(0, 26, 15, 463, 576, 15, 17)]]\n",
      "Kann Swar List: [[], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n",
      "----------------------------------------\n",
      "Subgroup Range: (29, 33)\n",
      "Swar List: [[(0, 30, 1, 113, 663, 11, 12)], [(0, 30, 2, 133, 658, 21, 12)], [(0, 30, 3, 163, 657, 8, 19)], [(0, 30, 4, 181, 658, 15, 17)], [(0, 30, 5, 203, 658, 15, 17)], [(0, 30, 6, 239, 663, 10, 12)], [(0, 30, 7, 265, 658, 19, 14)], [(0, 30, 8, 303, 662, 10, 13)], [(0, 30, 10, 336, 662, 11, 14)], [(0, 30, 11, 370, 667, 10, 4)]]\n",
      "Kann Swar List: [[(0, 29, 1, 112, 645, 10, 10)], [], [], [], [], [(0, 29, 6, 233, 645, 10, 9)], [], [(0, 29, 8, 298, 646, 7, 10)], [(0, 29, 10, 332, 646, 9, 10)], []]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the path to the folder containing the images\n",
    "image_folder_path = 'Analysis/bilawal_dhamaar'\n",
    "\n",
    "# Define the section-wise row numbers\n",
    "articulation_rows = [12, 14, 31, 33]\n",
    "kann_swar_rows = [4, 10, 16, 22, 29]\n",
    "swar_rows = [5, 7, 11, 18, 23, 26, 30]\n",
    "lyrics_rows = [6, 8, 13, 19, 24, 27, 32]\n",
    "\n",
    "# Define the subgroup ranges\n",
    "subgroup_ranges = [\n",
    "    (4, 7),\n",
    "    (7, 8),\n",
    "    (10, 14),\n",
    "    (16, 19),\n",
    "    (22, 24),\n",
    "    (26, 27),\n",
    "    (29, 33)\n",
    "]\n",
    "\n",
    "# Function to extract information from the image filename\n",
    "def extract_info_from_filename(filename):\n",
    "    pattern = r'(\\d+)_row(\\d+)(?:_col(\\d+))?_x(\\d+)_y(\\d+)_w(\\d+)_h(\\d+)'\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        page_num = int(match.group(1))\n",
    "        row_num = int(match.group(2))\n",
    "        col_num = int(match.group(3)) if match.group(3) else None\n",
    "        x = int(match.group(4))\n",
    "        y = int(match.group(5))\n",
    "        width = int(match.group(6))\n",
    "        height = int(match.group(7))\n",
    "        return page_num, row_num, col_num, x, y, width, height\n",
    "    return None\n",
    "\n",
    "# Load all image filenames and extract their information\n",
    "image_files = os.listdir(image_folder_path)\n",
    "image_info = [extract_info_from_filename(f) for f in image_files]\n",
    "image_info = [info for info in image_info if info is not None]\n",
    "\n",
    "# Organize images by row and column\n",
    "row_col_images = defaultdict(lambda: defaultdict(list))\n",
    "for info in image_info:\n",
    "    page_num, row_num, col_num, x, y, width, height = info\n",
    "    row_col_images[row_num][col_num].append(info)\n",
    "\n",
    "# Function to process a subgroup and create the lists of lists\n",
    "def process_subgroup(subgroup_range):\n",
    "    start_row, end_row = subgroup_range\n",
    "    \n",
    "    # Find the swar row in this subgroup\n",
    "    swar_row = None\n",
    "    for row in swar_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            swar_row = row\n",
    "            break\n",
    "    \n",
    "    if not swar_row:\n",
    "        return None, None\n",
    "    \n",
    "    # Find the kann swar row in this subgroup\n",
    "    kann_swar_row = None\n",
    "    for row in kann_swar_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            kann_swar_row = row\n",
    "            break\n",
    "    \n",
    "    # Find the articulation and lyrics rows (if any)\n",
    "    articulation_row = None\n",
    "    for row in articulation_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            articulation_row = row\n",
    "            break\n",
    "    \n",
    "    lyrics_row = None\n",
    "    for row in lyrics_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            lyrics_row = row\n",
    "            break\n",
    "    \n",
    "    # Get the swar images and their column numbers\n",
    "    swar_images = row_col_images[swar_row]\n",
    "    swar_cols = sorted(swar_images.keys())\n",
    "    \n",
    "    # Get the kann swar images and their column numbers\n",
    "    kann_swar_images = row_col_images[kann_swar_row] if kann_swar_row else {}\n",
    "    kann_swar_cols = sorted(kann_swar_images.keys())\n",
    "    \n",
    "    # Create the lists of lists\n",
    "    swar_list = []\n",
    "    kann_swar_list = []\n",
    "    \n",
    "    # Iterate through swar columns\n",
    "    swar_index = 0\n",
    "    kann_swar_index = 0\n",
    "    \n",
    "    while swar_index < len(swar_cols) or kann_swar_index < len(kann_swar_cols):\n",
    "        swar_col = swar_cols[swar_index] if swar_index < len(swar_cols) else None\n",
    "        kann_swar_col = kann_swar_cols[kann_swar_index] if kann_swar_index < len(kann_swar_cols) else None\n",
    "        \n",
    "        # If both columns exist and match\n",
    "        if swar_col is not None and kann_swar_col is not None and swar_col == kann_swar_col:\n",
    "            swar_list.append(swar_images[swar_col])\n",
    "            kann_swar_list.append(kann_swar_images[kann_swar_col])\n",
    "            swar_index += 1\n",
    "            kann_swar_index += 1\n",
    "        # If swar column exists but kann swar column doesn't match or is missing\n",
    "        elif swar_col is not None and (kann_swar_col is None or swar_col < kann_swar_col):\n",
    "            swar_list.append(swar_images[swar_col])\n",
    "            kann_swar_list.append([])\n",
    "            swar_index += 1\n",
    "        # If kann swar column exists but swar column doesn't match or is missing\n",
    "        elif kann_swar_col is not None and (swar_col is None or kann_swar_col < swar_col):\n",
    "            # Assign the kann swar to the next available swar column\n",
    "            if swar_index < len(swar_cols):\n",
    "                swar_list.append(swar_images[swar_cols[swar_index]])\n",
    "                kann_swar_list.append(kann_swar_images[kann_swar_col])\n",
    "                swar_index += 1\n",
    "                kann_swar_index += 1\n",
    "            else:\n",
    "                # If no more swar columns are available, append an empty list\n",
    "                swar_list.append([])\n",
    "                kann_swar_list.append(kann_swar_images[kann_swar_col])\n",
    "                kann_swar_index += 1\n",
    "    \n",
    "    return swar_list, kann_swar_list\n",
    "\n",
    "# Process each subgroup and store the results\n",
    "subgroup_results = {}\n",
    "for subgroup_range in subgroup_ranges:\n",
    "    swar_list, kann_swar_list = process_subgroup(subgroup_range)\n",
    "    if swar_list and kann_swar_list:\n",
    "        subgroup_results[subgroup_range] = {\n",
    "            'swar_list': swar_list,\n",
    "            'kann_swar_list': kann_swar_list\n",
    "        }\n",
    "\n",
    "# Print the results for each subgroup\n",
    "for subgroup_range, results in subgroup_results.items():\n",
    "    print(f\"Subgroup Range: {subgroup_range}\")\n",
    "    print(f\"Swar List: {results['swar_list']}\")\n",
    "    print(f\"Kann Swar List: {results['kann_swar_list']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroup Range: (4, 7)\n",
      "Swar List: [[(0, 5, 12, 406, 162, 10, 12)], [(0, 5, 13, 439, 162, 10, 14)], [(0, 5, 14, 472, 166, 9, 5)]]\n",
      "Kann Swar List: [[(0, 4, 12, 400, 145, 7, 10)], [(0, 4, 13, 434, 145, 9, 10)], []]\n",
      "----------------------------------------\n",
      "Subgroup Range: (7, 8)\n",
      "Swar List: [[(0, 7, 1, 118, 231, 9, 13)], [(0, 7, 2, 139, 236, 9, 4)], [(0, 7, 3, 159, 227, 14, 16)], [(0, 7, 4, 181, 214, 17, 29)], [(0, 7, 5, 215, 227, 15, 16)], [(0, 7, 6, 240, 235, 10, 5)], [(0, 7, 7, 260, 235, 11, 5)], [(0, 7, 8, 281, 226, 14, 17)], [(0, 7, 9, 304, 226, 14, 16)], [(0, 7, 10, 339, 227, 15, 15)], [(0, 7, 11, 376, 235, 9, 4)], [(0, 7, 12, 405, 225, 7, 17)], [(0, 7, 13, 434, 226, 15, 15)], [(0, 7, 14, 470, 234, 10, 4)]]\n",
      "Kann Swar List: [[], [], [(0, 7, 3, 156, 211, 14, 12)], [], [], [], [], [], [(0, 7, 9, 303, 213, 8, 8)], [], [], [(0, 7, 12, 401, 211, 13, 10)], [], []]\n",
      "----------------------------------------\n",
      "Subgroup Range: (10, 14)\n",
      "Swar List: [[(0, 11, 1, 117, 314, 10, 12)], [(0, 11, 2, 137, 309, 22, 23)], [(0, 11, 3, 169, 313, 8, 13)], [(0, 11, 4, 189, 318, 9, 4)], [(0, 11, 5, 215, 312, 10, 13)], [(0, 11, 6, 235, 313, 10, 12)], [(0, 11, 7, 255, 307, 15, 14)], [(0, 11, 8, 285, 312, 10, 13)], [(0, 11, 9, 309, 312, 9, 12)], [(0, 11, 10, 338, 312, 9, 12)], [(0, 11, 11, 367, 311, 18, 9)], [(0, 11, 12, 405, 312, 10, 13)], [(0, 11, 13, 437, 305, 8, 19)], [(0, 11, 14, 471, 316, 10, 4)]]\n",
      "Kann Swar List: [[(0, 10, 1, 116, 294, 14, 13)], [], [], [], [(0, 10, 5, 213, 296, 6, 9)], [(0, 10, 6, 232, 296, 7, 9)], [], [(0, 10, 8, 278, 295, 9, 9)], [], [], [], [], [], []]\n",
      "----------------------------------------\n",
      "Subgroup Range: (16, 19)\n",
      "Swar List: [[(0, 18, 1, 116, 395, 15, 12)], [(0, 18, 2, 142, 400, 10, 4)], [(0, 18, 3, 162, 400, 11, 4)], [(0, 18, 4, 183, 395, 15, 12)], [(0, 18, 5, 215, 392, 14, 15)], [(0, 18, 7, 239, 391, 15, 16)], [(0, 18, 8, 263, 399, 10, 5)], [(0, 18, 10, 284, 395, 10, 12)], [(0, 18, 11, 308, 399, 9, 5)], [(0, 18, 12, 338, 391, 13, 19)], [(0, 18, 13, 372, 394, 9, 13)], [(0, 18, 14, 404, 394, 10, 13)], [(0, 18, 15, 437, 394, 10, 13)], [(0, 18, 17, 468, 399, 12, 4)]]\n",
      "Kann Swar List: [[], [], [], [], [(0, 16, 5, 213, 374, 12, 14)], [], [], [(0, 16, 9, 273, 376, 14, 12)], [], [], [], [(0, 16, 14, 399, 377, 6, 10)], [(0, 16, 15, 438, 377, 9, 10)], []]\n",
      "----------------------------------------\n",
      "Subgroup Range: (22, 24)\n",
      "Swar List: [[(0, 23, 1, 115, 495, 14, 15)], [(0, 23, 2, 137, 502, 10, 5)], [(0, 23, 4, 158, 502, 10, 5)], [(0, 23, 6, 179, 494, 14, 16)], [(0, 23, 8, 204, 493, 14, 17)], [(0, 23, 9, 239, 494, 15, 16)], [(0, 23, 10, 275, 502, 9, 5)], [(0, 23, 11, 306, 493, 15, 17)], [(0, 23, 12, 336, 503, 9, 4)], [(0, 23, 13, 364, 494, 15, 16)], [(0, 23, 14, 403, 494, 15, 16)], [(0, 23, 15, 424, 493, 8, 18)], [(0, 23, 16, 442, 493, 15, 17)], [(0, 23, 17, 467, 502, 12, 4)]]\n",
      "Kann Swar List: [[(0, 22, 1, 114, 477, 12, 14)], [], [], [(0, 22, 6, 173, 478, 13, 12)], [(0, 22, 7, 194, 481, 9, 10)], [], [], [], [], [], [(0, 22, 14, 397, 477, 12, 13)], [], [], []]\n",
      "----------------------------------------\n",
      "Subgroup Range: (26, 27)\n",
      "Swar List: [[(0, 26, 1, 114, 577, 15, 16)], [(0, 26, 2, 140, 576, 10, 17)], [(0, 26, 3, 161, 585, 9, 4)], [(0, 26, 4, 179, 558, 8, 16)], [(0, 26, 5, 185, 576, 10, 18)], [(0, 26, 6, 208, 585, 10, 4)], [(0, 26, 7, 238, 585, 10, 5)], [(0, 26, 8, 274, 577, 10, 16)], [(0, 26, 9, 300, 561, 12, 31)], [(0, 26, 10, 337, 585, 10, 4)], [(0, 26, 11, 368, 585, 12, 4)], [(0, 26, 12, 402, 576, 15, 17)], [(0, 26, 13, 424, 575, 7, 18)], [(0, 26, 14, 441, 575, 15, 18)], [(0, 26, 15, 463, 576, 15, 17)]]\n",
      "Kann Swar List: [[(0, 26, 1, 113, 560, 13, 13)], [], [], [], [], [], [], [], [], [], [], [(0, 26, 12, 397, 560, 12, 13)], [], [], []]\n",
      "----------------------------------------\n",
      "Subgroup Range: (29, 33)\n",
      "Swar List: [[(0, 30, 1, 113, 663, 11, 12)], [(0, 30, 2, 133, 658, 21, 12)], [(0, 30, 3, 163, 657, 8, 19)], [(0, 30, 4, 181, 658, 15, 17)], [(0, 30, 5, 203, 658, 15, 17)], [(0, 30, 6, 239, 663, 10, 12)], [(0, 30, 7, 265, 658, 19, 14)], [(0, 30, 8, 303, 662, 10, 13)], [(0, 30, 10, 336, 662, 11, 14)], [(0, 30, 11, 370, 667, 10, 4)]]\n",
      "Kann Swar List: [[(0, 29, 1, 112, 645, 10, 10)], [], [], [], [], [(0, 29, 6, 233, 645, 10, 9)], [], [(0, 29, 8, 298, 646, 7, 10)], [(0, 29, 10, 332, 646, 9, 10)], []]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the path to the folder containing the images\n",
    "image_folder_path = 'Analysis/bilawal_dhamaar'\n",
    "\n",
    "# Define the section-wise row numbers\n",
    "articulation_rows = [12, 14, 31, 33]\n",
    "kann_swar_rows = [4, 10, 16, 22, 29]\n",
    "swar_rows = [5, 7, 11, 18, 23, 26, 30]\n",
    "lyrics_rows = [6, 8, 13, 19, 24, 27, 32]\n",
    "\n",
    "# Define the subgroup ranges\n",
    "subgroup_ranges = [\n",
    "    (4, 7),\n",
    "    (7, 8),\n",
    "    (10, 14),\n",
    "    (16, 19),\n",
    "    (22, 24),\n",
    "    (26, 27),\n",
    "    (29, 33)\n",
    "]\n",
    "\n",
    "# Function to extract information from the image filename\n",
    "def extract_info_from_filename(filename):\n",
    "    pattern = r'(\\d+)_row(\\d+)(?:_col(\\d+))?_x(\\d+)_y(\\d+)_w(\\d+)_h(\\d+)'\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        page_num = int(match.group(1))\n",
    "        row_num = int(match.group(2))\n",
    "        col_num = int(match.group(3)) if match.group(3) else None\n",
    "        x = int(match.group(4))\n",
    "        y = int(match.group(5))\n",
    "        width = int(match.group(6))\n",
    "        height = int(match.group(7))\n",
    "        return page_num, row_num, col_num, x, y, width, height\n",
    "    return None\n",
    "\n",
    "# Load all image filenames and extract their information\n",
    "image_files = os.listdir(image_folder_path)\n",
    "image_info = [extract_info_from_filename(f) for f in image_files]\n",
    "image_info = [info for info in image_info if info is not None]\n",
    "\n",
    "# Organize images by row and column\n",
    "row_col_images = defaultdict(lambda: defaultdict(list))\n",
    "for info in image_info:\n",
    "    page_num, row_num, col_num, x, y, width, height = info\n",
    "    row_col_images[row_num][col_num].append(info)\n",
    "\n",
    "# Function to process a subgroup and create the lists of lists\n",
    "def process_subgroup(subgroup_range):\n",
    "    start_row, end_row = subgroup_range\n",
    "    \n",
    "    # Find the swar row in this subgroup\n",
    "    swar_row = None\n",
    "    for row in swar_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            swar_row = row\n",
    "            break\n",
    "    \n",
    "    if not swar_row:\n",
    "        return None, None\n",
    "    \n",
    "    # Find the kann swar row in this subgroup\n",
    "    kann_swar_row = None\n",
    "    for row in kann_swar_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            kann_swar_row = row\n",
    "            break\n",
    "    \n",
    "    # Find the articulation and lyrics rows (if any)\n",
    "    articulation_row = None\n",
    "    for row in articulation_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            articulation_row = row\n",
    "            break\n",
    "    \n",
    "    lyrics_row = None\n",
    "    for row in lyrics_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            lyrics_row = row\n",
    "            break\n",
    "    \n",
    "    # Get the swar images and their column numbers\n",
    "    swar_images = row_col_images[swar_row]\n",
    "    swar_cols = sorted(swar_images.keys())\n",
    "    \n",
    "    # Get the kann swar images and their column numbers (if kann swar row exists)\n",
    "    kann_swar_images = row_col_images[kann_swar_row] if kann_swar_row else {}\n",
    "    kann_swar_cols = sorted(kann_swar_images.keys())\n",
    "    \n",
    "    # Create the lists of lists\n",
    "    swar_list = []\n",
    "    kann_swar_list = []\n",
    "    \n",
    "    # Case 1: If there is an explicit kann swar row\n",
    "    if kann_swar_row:\n",
    "        swar_index = 0\n",
    "        kann_swar_index = 0\n",
    "        \n",
    "        while swar_index < len(swar_cols) or kann_swar_index < len(kann_swar_cols):\n",
    "            swar_col = swar_cols[swar_index] if swar_index < len(swar_cols) else None\n",
    "            kann_swar_col = kann_swar_cols[kann_swar_index] if kann_swar_index < len(kann_swar_cols) else None\n",
    "            \n",
    "            # If both columns exist and match\n",
    "            if swar_col is not None and kann_swar_col is not None and swar_col == kann_swar_col:\n",
    "                swar_list.append(swar_images[swar_col])\n",
    "                kann_swar_list.append(kann_swar_images[kann_swar_col])\n",
    "                swar_index += 1\n",
    "                kann_swar_index += 1\n",
    "            # If swar column exists but kann swar column doesn't match or is missing\n",
    "            elif swar_col is not None and (kann_swar_col is None or swar_col < kann_swar_col):\n",
    "                swar_list.append(swar_images[swar_col])\n",
    "                kann_swar_list.append([])\n",
    "                swar_index += 1\n",
    "            # If kann swar column exists but swar column doesn't match or is missing\n",
    "            elif kann_swar_col is not None and (swar_col is None or kann_swar_col < swar_col):\n",
    "                # Assign the kann swar to the next available swar column\n",
    "                if swar_index < len(swar_cols):\n",
    "                    swar_list.append(swar_images[swar_cols[swar_index]])\n",
    "                    kann_swar_list.append(kann_swar_images[kann_swar_col])\n",
    "                    swar_index += 1\n",
    "                    kann_swar_index += 1\n",
    "                else:\n",
    "                    # If no more swar columns are available, append an empty list\n",
    "                    swar_list.append([])\n",
    "                    kann_swar_list.append(kann_swar_images[kann_swar_col])\n",
    "                    kann_swar_index += 1\n",
    "    \n",
    "    # Case 2: If there is no explicit kann swar row, check for hidden kann swars in the swar row\n",
    "    else:\n",
    "        for col in swar_cols:\n",
    "            images_in_col = swar_images[col]\n",
    "            if len(images_in_col) == 1:\n",
    "                # Only one image in this column, so no hidden kann swar\n",
    "                swar_list.append(images_in_col)\n",
    "                kann_swar_list.append([])\n",
    "            else:\n",
    "                # Multiple images in the same column, so identify hidden kann swars\n",
    "                # Sort images by y-value (lower y-value is kann swar)\n",
    "                sorted_images = sorted(images_in_col, key=lambda x: x[4])  # Sort by y-value\n",
    "                kann_swar_list.append([sorted_images[0]])  # Lower y-value is kann swar\n",
    "                swar_list.append([sorted_images[1]])  # Higher y-value is swar\n",
    "    \n",
    "    return swar_list, kann_swar_list\n",
    "\n",
    "# Process each subgroup and store the results\n",
    "subgroup_results = {}\n",
    "for subgroup_range in subgroup_ranges:\n",
    "    swar_list, kann_swar_list = process_subgroup(subgroup_range)\n",
    "    if swar_list and kann_swar_list:\n",
    "        subgroup_results[subgroup_range] = {\n",
    "            'swar_list': swar_list,\n",
    "            'kann_swar_list': kann_swar_list\n",
    "        }\n",
    "\n",
    "# Print the results for each subgroup\n",
    "for subgroup_range, results in subgroup_results.items():\n",
    "    print(f\"Subgroup Range: {subgroup_range}\")\n",
    "    print(f\"Swar List: {results['swar_list']}\")\n",
    "    print(f\"Kann Swar List: {results['kann_swar_list']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroup Range: (4, 7)\n",
      "Kann Swar List: [[], [], [], [], [], [], [], [], [], [], [], [(0, 4, 12, 400, 145, 7, 10)], [(0, 4, 13, 434, 145, 9, 10)], []]\n",
      "Swar List: [[], [], [], [], [], [], [], [], [], [], [], [(0, 5, 12, 406, 162, 10, 12)], [(0, 5, 13, 439, 162, 10, 14)], [(0, 5, 14, 472, 166, 9, 5)]]\n",
      "Swar Articulation Checks: [[], [], [], [], [], [], [], [], [], [], [], False, False, False]\n",
      "Lyrics List: [[], [], [], [], [], [], [], [], [], [], [], [(0, 6, 12, 405, 188, 11, 15)], [(0, 6, 13, 436, 188, 14, 12)], [(0, 6, 14, 473, 187, 8, 12)]]\n",
      "Lyrics Articulation Checks: [[], [], [], [], [], [], [], [], [], [], [], False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (7, 8)\n",
      "Kann Swar List: [[], [], [(0, 7, 3, 156, 211, 14, 12)], [], [], [], [], [], [(0, 7, 9, 303, 213, 8, 8)], [], [], [(0, 7, 12, 401, 211, 13, 10)], [], []]\n",
      "Swar List: [[(0, 7, 1, 118, 231, 9, 13)], [(0, 7, 2, 139, 236, 9, 4)], [(0, 7, 3, 159, 227, 14, 16)], [(0, 7, 4, 181, 214, 17, 29)], [(0, 7, 5, 215, 227, 15, 16)], [(0, 7, 6, 240, 235, 10, 5)], [(0, 7, 7, 260, 235, 11, 5)], [(0, 7, 8, 281, 226, 14, 17)], [(0, 7, 9, 304, 226, 14, 16)], [(0, 7, 10, 339, 227, 15, 15)], [(0, 7, 11, 376, 235, 9, 4)], [(0, 7, 12, 405, 225, 7, 17)], [(0, 7, 13, 434, 226, 15, 15)], [(0, 7, 14, 470, 234, 10, 4)]]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics List: [[(0, 8, 1, 118, 256, 12, 17)], [(0, 8, 2, 141, 260, 8, 12)], [(0, 8, 3, 163, 260, 9, 13)], [(0, 8, 4, 184, 260, 11, 12)], [(0, 8, 5, 215, 259, 13, 12)], [(0, 8, 6, 242, 259, 8, 12)], [(0, 8, 7, 263, 259, 8, 12)], [(0, 8, 8, 285, 258, 10, 13)], [(0, 8, 9, 310, 259, 9, 12)], [(0, 8, 10, 339, 252, 14, 19)], [(0, 8, 11, 375, 258, 8, 12)], [(0, 8, 12, 406, 258, 8, 14)], [(0, 8, 13, 441, 258, 10, 13)], [(0, 8, 14, 473, 257, 7, 13)]]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (10, 14)\n",
      "Kann Swar List: [[(0, 10, 1, 116, 294, 14, 13)], [], [], [], [(0, 10, 5, 213, 296, 6, 9)], [(0, 10, 6, 232, 296, 7, 9)], [], [(0, 10, 8, 278, 295, 9, 9)], [], [], [], [], [], []]\n",
      "Swar List: [[(0, 11, 1, 117, 314, 10, 12)], [(0, 11, 2, 137, 309, 22, 23)], [(0, 11, 3, 169, 313, 8, 13)], [(0, 11, 4, 189, 318, 9, 4)], [(0, 11, 5, 215, 312, 10, 13)], [(0, 11, 6, 235, 313, 10, 12)], [(0, 11, 7, 255, 307, 15, 14)], [(0, 11, 8, 285, 312, 10, 13)], [(0, 11, 9, 309, 312, 9, 12)], [(0, 11, 10, 338, 312, 9, 12)], [(0, 11, 11, 367, 311, 18, 9)], [(0, 11, 12, 405, 312, 10, 13)], [(0, 11, 13, 437, 305, 8, 19)], [(0, 11, 14, 471, 316, 10, 4)]]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, True, False, False, False, True, False, False, False]\n",
      "Lyrics List: [[(0, 13, 1, 117, 343, 11, 14)], [(0, 13, 2, 137, 337, 20, 15)], [(0, 13, 3, 165, 342, 15, 14)], [(0, 13, 4, 190, 342, 8, 12)], [(0, 13, 5, 215, 342, 10, 11)], [(0, 13, 6, 234, 342, 9, 12)], [(0, 13, 7, 256, 341, 13, 10)], [(0, 13, 8, 282, 342, 15, 11)], [(0, 13, 9, 311, 341, 7, 12)], [(0, 13, 10, 338, 342, 11, 11)], [(0, 13, 11, 367, 340, 15, 19)], [(0, 13, 12, 405, 341, 9, 13)], [(0, 13, 13, 436, 341, 9, 12)], [(0, 13, 14, 473, 340, 7, 12)]]\n",
      "Lyrics Articulation Checks: [False, True, False, False, False, False, True, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (16, 19)\n",
      "Kann Swar List: [[], [], [], [], [(0, 16, 5, 213, 374, 12, 14)], [], [], [(0, 16, 9, 273, 376, 14, 12)], [], [], [], [(0, 16, 14, 399, 377, 6, 10)], [(0, 16, 15, 438, 377, 9, 10)], []]\n",
      "Swar List: [[(0, 18, 1, 116, 395, 15, 12)], [(0, 18, 2, 142, 400, 10, 4)], [(0, 18, 3, 162, 400, 11, 4)], [(0, 18, 4, 183, 395, 15, 12)], [(0, 18, 5, 215, 392, 14, 15)], [(0, 18, 7, 239, 391, 15, 16)], [(0, 18, 8, 263, 399, 10, 5)], [(0, 18, 10, 284, 395, 10, 12)], [(0, 18, 11, 308, 399, 9, 5)], [(0, 18, 12, 338, 391, 13, 19)], [(0, 18, 13, 372, 394, 9, 13)], [(0, 18, 14, 404, 394, 10, 13)], [(0, 18, 15, 437, 394, 10, 13)], [(0, 18, 17, 468, 399, 12, 4)]]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics List: [[(0, 19, 1, 115, 418, 15, 18)], [(0, 19, 2, 141, 423, 7, 13)], [(0, 19, 3, 165, 423, 8, 13)], [(0, 19, 4, 183, 424, 11, 11)], [(0, 19, 5, 214, 420, 9, 16)], [(0, 19, 7, 237, 423, 8, 12)], [(0, 19, 8, 263, 423, 7, 12)], [(0, 19, 10, 288, 424, 9, 14)], [(0, 19, 11, 310, 423, 8, 12)], [(0, 19, 12, 338, 423, 7, 12)], [(0, 19, 13, 367, 424, 13, 11)], [(0, 19, 14, 402, 417, 13, 22)], [(0, 19, 15, 432, 423, 13, 13)], []]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (22, 24)\n",
      "Kann Swar List: [[(0, 22, 1, 114, 477, 12, 14)], [], [], [(0, 22, 6, 173, 478, 13, 12)], [(0, 22, 7, 194, 481, 9, 10)], [], [], [], [], [], [(0, 22, 14, 397, 477, 12, 13)], [], [], []]\n",
      "Swar List: [[(0, 23, 1, 115, 495, 14, 15)], [(0, 23, 2, 137, 502, 10, 5)], [(0, 23, 4, 158, 502, 10, 5)], [(0, 23, 6, 179, 494, 14, 16)], [(0, 23, 8, 204, 493, 14, 17)], [(0, 23, 9, 239, 494, 15, 16)], [(0, 23, 10, 275, 502, 9, 5)], [(0, 23, 11, 306, 493, 15, 17)], [(0, 23, 12, 336, 503, 9, 4)], [(0, 23, 13, 364, 494, 15, 16)], [(0, 23, 14, 403, 494, 15, 16)], [(0, 23, 15, 424, 493, 8, 18)], [(0, 23, 16, 442, 493, 15, 17)], [(0, 23, 17, 467, 502, 12, 4)]]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics List: [[(0, 24, 1, 115, 527, 13, 12)], [], [], [(0, 24, 6, 185, 527, 10, 12)], [(0, 24, 8, 209, 527, 9, 13)], [(0, 24, 9, 238, 527, 14, 12)], [(0, 24, 10, 274, 526, 8, 13)], [(0, 24, 11, 304, 527, 12, 12)], [(0, 24, 12, 337, 527, 8, 11)], [(0, 24, 13, 369, 527, 11, 15)], [(0, 24, 14, 403, 522, 9, 21)], [(0, 24, 15, 423, 526, 7, 13)], [(0, 24, 16, 449, 527, 10, 13)], [(0, 24, 17, 471, 526, 8, 12)]]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (26, 27)\n",
      "Kann Swar List: [[(0, 26, 1, 113, 560, 13, 13)], [], [], [], [], [], [], [], [], [], [], [(0, 26, 12, 397, 560, 12, 13)], [], [], []]\n",
      "Swar List: [[(0, 26, 1, 114, 577, 15, 16)], [(0, 26, 2, 140, 576, 10, 17)], [(0, 26, 3, 161, 585, 9, 4)], [(0, 26, 4, 179, 558, 8, 16)], [(0, 26, 5, 185, 576, 10, 18)], [(0, 26, 6, 208, 585, 10, 4)], [(0, 26, 7, 238, 585, 10, 5)], [(0, 26, 8, 274, 577, 10, 16)], [(0, 26, 9, 300, 561, 12, 31)], [(0, 26, 10, 337, 585, 10, 4)], [(0, 26, 11, 368, 585, 12, 4)], [(0, 26, 12, 402, 576, 15, 17)], [(0, 26, 13, 424, 575, 7, 18)], [(0, 26, 14, 441, 575, 15, 18)], [(0, 26, 15, 463, 576, 15, 17)]]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics List: [[(0, 27, 1, 114, 604, 13, 17)], [(0, 27, 2, 142, 609, 8, 11)], [(0, 27, 3, 160, 608, 8, 13)], [], [(0, 27, 5, 186, 610, 9, 12)], [(0, 27, 6, 210, 609, 8, 12)], [(0, 27, 7, 238, 609, 8, 11)], [(0, 27, 8, 274, 609, 10, 17)], [(0, 27, 9, 304, 609, 9, 13)], [(0, 27, 10, 341, 609, 8, 12)], [(0, 27, 11, 372, 609, 7, 12)], [(0, 27, 12, 402, 603, 10, 18)], [(0, 27, 13, 427, 609, 7, 12)], [(0, 27, 14, 446, 609, 9, 13)], [(0, 27, 15, 470, 608, 8, 14)]]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (29, 33)\n",
      "Kann Swar List: [[(0, 29, 1, 112, 645, 10, 10)], [], [], [], [], [(0, 29, 6, 233, 645, 10, 9)], [], [(0, 29, 8, 298, 646, 7, 10)], [(0, 29, 10, 332, 646, 9, 10)], [], [], [], [], []]\n",
      "Swar List: [[(0, 30, 1, 113, 663, 11, 12)], [(0, 30, 2, 133, 658, 21, 12)], [(0, 30, 3, 163, 657, 8, 19)], [(0, 30, 4, 181, 658, 15, 17)], [(0, 30, 5, 203, 658, 15, 17)], [(0, 30, 6, 239, 663, 10, 12)], [(0, 30, 7, 265, 658, 19, 14)], [(0, 30, 8, 303, 662, 10, 13)], [(0, 30, 10, 336, 662, 11, 14)], [(0, 30, 11, 370, 667, 10, 4)], [], [], [], []]\n",
      "Swar Articulation Checks: [False, True, False, False, False, False, True, False, False, False, False, False, False, False]\n",
      "Lyrics List: [[(0, 32, 1, 113, 688, 15, 16)], [(0, 32, 2, 138, 691, 13, 11)], [(0, 32, 3, 167, 692, 7, 12)], [(0, 32, 4, 185, 692, 12, 13)], [(0, 32, 5, 207, 692, 10, 12)], [(0, 32, 6, 238, 691, 15, 13)], [(0, 32, 7, 270, 690, 13, 11)], [(0, 32, 8, 304, 691, 8, 12)], [(0, 32, 10, 334, 691, 14, 13)], [(0, 32, 11, 371, 691, 8, 12)], [], [], [], []]\n",
      "Lyrics Articulation Checks: [False, True, False, False, False, False, True, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# this code is working fine, but when lyrics have little different column numbers from  \n",
    "# swar row, then it is not mapping perfectly\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the path to the folder containing the images\n",
    "image_folder_path = 'Analysis/bilawal_dhamaar'\n",
    "\n",
    "# Define the section-wise row numbers\n",
    "articulation_rows = [12, 14, 31, 33]\n",
    "kann_swar_rows = [4, 10, 16, 22, 29]\n",
    "swar_rows = [5, 7, 11, 18, 23, 26, 30]\n",
    "lyrics_rows = [6, 8, 13, 19, 24, 27, 32]\n",
    "\n",
    "# Define the subgroup ranges\n",
    "subgroup_ranges = [\n",
    "    (4, 7),\n",
    "    (7, 8),\n",
    "    (10, 14),\n",
    "    (16, 19),\n",
    "    (22, 24),\n",
    "    (26, 27),\n",
    "    (29, 33)\n",
    "]\n",
    "\n",
    "# Define the beat count (size of the lists)\n",
    "beat_count = 14\n",
    "\n",
    "# Function to extract information from the image filename\n",
    "def extract_info_from_filename(filename):\n",
    "    pattern = r'(\\d+)_row(\\d+)(?:_col(\\d+))?_x(\\d+)_y(\\d+)_w(\\d+)_h(\\d+)'\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        page_num = int(match.group(1))\n",
    "        row_num = int(match.group(2))\n",
    "        col_num = int(match.group(3)) if match.group(3) else None\n",
    "        x = int(match.group(4))\n",
    "        y = int(match.group(5))\n",
    "        width = int(match.group(6))\n",
    "        height = int(match.group(7))\n",
    "        return page_num, row_num, col_num, x, y, width, height\n",
    "    return None\n",
    "\n",
    "# Load all image filenames and extract their information\n",
    "image_files = os.listdir(image_folder_path)\n",
    "image_info = [extract_info_from_filename(f) for f in image_files]\n",
    "image_info = [info for info in image_info if info is not None]\n",
    "\n",
    "# Organize images by row and column\n",
    "row_col_images = defaultdict(lambda: defaultdict(list))\n",
    "for info in image_info:\n",
    "    page_num, row_num, col_num, x, y, width, height = info\n",
    "    row_col_images[row_num][col_num].append(info)\n",
    "\n",
    "# Function to pad lists to match the beat count\n",
    "def pad_lists(lists, size):\n",
    "    if len(lists) < size:\n",
    "        padding = [[] for _ in range(size - len(lists))]\n",
    "        return padding + lists\n",
    "    return lists\n",
    "\n",
    "# Function to process a subgroup and create the lists of lists\n",
    "def process_subgroup(subgroup_range, is_first_subgroup):\n",
    "    start_row, end_row = subgroup_range\n",
    "    \n",
    "    # Find the swar row in this subgroup\n",
    "    swar_row = None\n",
    "    for row in swar_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            swar_row = row\n",
    "            break\n",
    "    \n",
    "    if not swar_row:\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    # Find the kann swar row in this subgroup\n",
    "    kann_swar_row = None\n",
    "    for row in kann_swar_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            kann_swar_row = row\n",
    "            break\n",
    "    \n",
    "    # Find the articulation rows in this subgroup\n",
    "    articulation_rows_in_subgroup = [row for row in articulation_rows if start_row <= row <= end_row]\n",
    "    \n",
    "    # Find the lyrics row in this subgroup\n",
    "    lyrics_row = None\n",
    "    for row in lyrics_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            lyrics_row = row\n",
    "            break\n",
    "    \n",
    "    # Get the swar images and their column numbers\n",
    "    swar_images = row_col_images[swar_row]\n",
    "    swar_cols = sorted(swar_images.keys())\n",
    "    \n",
    "    # Get the kann swar images and their column numbers (if kann swar row exists)\n",
    "    kann_swar_images = row_col_images[kann_swar_row] if kann_swar_row else {}\n",
    "    kann_swar_cols = sorted(kann_swar_images.keys())\n",
    "    \n",
    "    # Get the lyrics images and their column numbers (if lyrics row exists)\n",
    "    lyrics_images = row_col_images[lyrics_row] if lyrics_row else {}\n",
    "    lyrics_cols = sorted(lyrics_images.keys())\n",
    "    \n",
    "    # Create the lists of lists\n",
    "    swar_list = []\n",
    "    kann_swar_list = []\n",
    "    swar_articulation_checks = [False] * len(swar_cols)\n",
    "    lyrics_articulation_checks = [False] * len(swar_cols)\n",
    "    lyrics_list = []\n",
    "    \n",
    "    # Case 1: If there is an explicit kann swar row\n",
    "    if kann_swar_row:\n",
    "        swar_index = 0\n",
    "        kann_swar_index = 0\n",
    "        \n",
    "        while swar_index < len(swar_cols) or kann_swar_index < len(kann_swar_cols):\n",
    "            swar_col = swar_cols[swar_index] if swar_index < len(swar_cols) else None\n",
    "            kann_swar_col = kann_swar_cols[kann_swar_index] if kann_swar_index < len(kann_swar_cols) else None\n",
    "            \n",
    "            # If both columns exist and match\n",
    "            if swar_col is not None and kann_swar_col is not None and swar_col == kann_swar_col:\n",
    "                swar_list.append(swar_images[swar_col])\n",
    "                kann_swar_list.append(kann_swar_images[kann_swar_col])\n",
    "                swar_index += 1\n",
    "                kann_swar_index += 1\n",
    "            # If swar column exists but kann swar column doesn't match or is missing\n",
    "            elif swar_col is not None and (kann_swar_col is None or swar_col < kann_swar_col):\n",
    "                swar_list.append(swar_images[swar_col])\n",
    "                kann_swar_list.append([])\n",
    "                swar_index += 1\n",
    "            # If kann swar column exists but swar column doesn't match or is missing\n",
    "            elif kann_swar_col is not None and (swar_col is None or kann_swar_col < swar_col):\n",
    "                # Assign the kann swar to the next available swar column\n",
    "                if swar_index < len(swar_cols):\n",
    "                    swar_list.append(swar_images[swar_cols[swar_index]])\n",
    "                    kann_swar_list.append(kann_swar_images[kann_swar_col])\n",
    "                    swar_index += 1\n",
    "                    kann_swar_index += 1\n",
    "                else:\n",
    "                    # If no more swar columns are available, append an empty list\n",
    "                    swar_list.append([])\n",
    "                    kann_swar_list.append(kann_swar_images[kann_swar_col])\n",
    "                    kann_swar_index += 1\n",
    "    \n",
    "    # Case 2: If there is no explicit kann swar row, check for hidden kann swars in the swar row\n",
    "    else:\n",
    "        for col in swar_cols:\n",
    "            images_in_col = swar_images[col]\n",
    "            if len(images_in_col) == 1:\n",
    "                # Only one image in this column, so no hidden kann swar\n",
    "                swar_list.append(images_in_col)\n",
    "                kann_swar_list.append([])\n",
    "            else:\n",
    "                # Multiple images in the same column, so identify hidden kann swars\n",
    "                # Sort images by y-value (lower y-value is kann swar)\n",
    "                sorted_images = sorted(images_in_col, key=lambda x: x[4])  # Sort by y-value\n",
    "                kann_swar_list.append([sorted_images[0]])  # Lower y-value is kann swar\n",
    "                swar_list.append([sorted_images[1]])  # Higher y-value is swar\n",
    "    \n",
    "    # Handle articulation rows\n",
    "    for articulation_row in articulation_rows_in_subgroup:\n",
    "        # Find the row just before the articulation row\n",
    "        prev_row = articulation_row - 1\n",
    "        if prev_row in swar_rows:\n",
    "            # Swar articulation\n",
    "            articulation_images = row_col_images[articulation_row]\n",
    "            articulation_cols = sorted(articulation_images.keys())\n",
    "            for i, col in enumerate(swar_cols):\n",
    "                if col in articulation_cols:\n",
    "                    swar_articulation_checks[i] = True\n",
    "        elif prev_row in lyrics_rows:\n",
    "            # Lyrics articulation\n",
    "            articulation_images = row_col_images[articulation_row]\n",
    "            articulation_cols = sorted(articulation_images.keys())\n",
    "            for i, col in enumerate(swar_cols):\n",
    "                if col in articulation_cols:\n",
    "                    lyrics_articulation_checks[i] = True\n",
    "    \n",
    "    # Handle lyrics row\n",
    "    if lyrics_row:\n",
    "        for col in swar_cols:\n",
    "            if col in lyrics_cols:\n",
    "                lyrics_list.append(lyrics_images[col])\n",
    "            else:\n",
    "                lyrics_list.append([])\n",
    "    else:\n",
    "        lyrics_list = [[] for _ in range(len(swar_cols))]\n",
    "    \n",
    "    # Pad lists to match the beat count\n",
    "    if is_first_subgroup:\n",
    "        swar_list = pad_lists(swar_list, beat_count)\n",
    "        kann_swar_list = pad_lists(kann_swar_list, beat_count)\n",
    "        swar_articulation_checks = pad_lists(swar_articulation_checks, beat_count)\n",
    "        lyrics_articulation_checks = pad_lists(lyrics_articulation_checks, beat_count)\n",
    "        lyrics_list = pad_lists(lyrics_list, beat_count)\n",
    "    else:\n",
    "        if len(swar_list) < beat_count:\n",
    "            swar_list += [[] for _ in range(beat_count - len(swar_list))]\n",
    "        if len(kann_swar_list) < beat_count:\n",
    "            kann_swar_list += [[] for _ in range(beat_count - len(kann_swar_list))]\n",
    "        if len(swar_articulation_checks) < beat_count:\n",
    "            swar_articulation_checks += [False for _ in range(beat_count - len(swar_articulation_checks))]\n",
    "        if len(lyrics_articulation_checks) < beat_count:\n",
    "            lyrics_articulation_checks += [False for _ in range(beat_count - len(lyrics_articulation_checks))]\n",
    "        if len(lyrics_list) < beat_count:\n",
    "            lyrics_list += [[] for _ in range(beat_count - len(lyrics_list))]\n",
    "    \n",
    "    return swar_list, kann_swar_list, swar_articulation_checks, lyrics_articulation_checks, lyrics_list\n",
    "\n",
    "# Process each subgroup and store the results\n",
    "subgroup_results = {}\n",
    "for i, subgroup_range in enumerate(subgroup_ranges):\n",
    "    is_first_subgroup = (i == 0)\n",
    "    swar_list, kann_swar_list, swar_articulation_checks, lyrics_articulation_checks, lyrics_list = process_subgroup(subgroup_range, is_first_subgroup)\n",
    "    if swar_list and kann_swar_list:\n",
    "        subgroup_results[subgroup_range] = {\n",
    "            'swar_list': swar_list,\n",
    "            'kann_swar_list': kann_swar_list,\n",
    "            'swar_articulation_checks': swar_articulation_checks,\n",
    "            'lyrics_articulation_checks': lyrics_articulation_checks,\n",
    "            'lyrics_list': lyrics_list\n",
    "        }\n",
    "\n",
    "# Print the results for each subgroup\n",
    "for subgroup_range, results in subgroup_results.items():\n",
    "    print(f\"Subgroup Range: {subgroup_range}\")\n",
    "    print(f\"Kann Swar List: {results['kann_swar_list']}\")\n",
    "    print(f\"Swar List: {results['swar_list']}\")\n",
    "    print(f\"Swar Articulation Checks: {results['swar_articulation_checks']}\")\n",
    "    print(f\"Lyrics List: {results['lyrics_list']}\")\n",
    "    print(f\"Lyrics Articulation Checks: {results['lyrics_articulation_checks']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroup Range: (4, 7)\n",
      "Kann Swar List: [[], [], [], [], [], [], [], [], [], [], [], [(0, 4, 12, 400, 145, 7, 10)], [(0, 4, 13, 434, 145, 9, 10)], []]\n",
      "Swar List: [[], [], [], [], [], [], [], [], [], [], [], [(0, 5, 12, 406, 162, 10, 12)], [(0, 5, 13, 439, 162, 10, 14)], [(0, 5, 14, 472, 166, 9, 5)]]\n",
      "Swar Articulation Checks: [[], [], [], [], [], [], [], [], [], [], [], False, False, False]\n",
      "Lyrics List: [[], [], [], [], [], [], [], [], [], [], [], [(0, 6, 12, 405, 188, 11, 15)], [(0, 6, 13, 436, 188, 14, 12)], [(0, 6, 14, 473, 187, 8, 12)]]\n",
      "Lyrics Articulation Checks: [[], [], [], [], [], [], [], [], [], [], [], False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (7, 8)\n",
      "Kann Swar List: [[], [], [(0, 7, 3, 156, 211, 14, 12)], [], [], [], [], [], [(0, 7, 9, 303, 213, 8, 8)], [], [], [(0, 7, 12, 401, 211, 13, 10)], [], []]\n",
      "Swar List: [[(0, 7, 1, 118, 231, 9, 13)], [(0, 7, 2, 139, 236, 9, 4)], [(0, 7, 3, 159, 227, 14, 16)], [(0, 7, 4, 181, 214, 17, 29)], [(0, 7, 5, 215, 227, 15, 16)], [(0, 7, 6, 240, 235, 10, 5)], [(0, 7, 7, 260, 235, 11, 5)], [(0, 7, 8, 281, 226, 14, 17)], [(0, 7, 9, 304, 226, 14, 16)], [(0, 7, 10, 339, 227, 15, 15)], [(0, 7, 11, 376, 235, 9, 4)], [(0, 7, 12, 405, 225, 7, 17)], [(0, 7, 13, 434, 226, 15, 15)], [(0, 7, 14, 470, 234, 10, 4)]]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics List: [[(0, 8, 1, 118, 256, 12, 17)], [(0, 8, 2, 141, 260, 8, 12)], [(0, 8, 3, 163, 260, 9, 13)], [(0, 8, 4, 184, 260, 11, 12)], [(0, 8, 5, 215, 259, 13, 12)], [(0, 8, 6, 242, 259, 8, 12)], [(0, 8, 7, 263, 259, 8, 12)], [(0, 8, 8, 285, 258, 10, 13)], [(0, 8, 9, 310, 259, 9, 12)], [(0, 8, 10, 339, 252, 14, 19)], [(0, 8, 11, 375, 258, 8, 12)], [(0, 8, 12, 406, 258, 8, 14)], [(0, 8, 13, 441, 258, 10, 13)], [(0, 8, 14, 473, 257, 7, 13)]]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (10, 14)\n",
      "Kann Swar List: [[(0, 10, 1, 116, 294, 14, 13)], [], [], [], [(0, 10, 5, 213, 296, 6, 9)], [(0, 10, 6, 232, 296, 7, 9)], [], [(0, 10, 8, 278, 295, 9, 9)], [], [], [], [], [], []]\n",
      "Swar List: [[(0, 11, 1, 117, 314, 10, 12)], [(0, 11, 2, 137, 309, 22, 23)], [(0, 11, 3, 169, 313, 8, 13)], [(0, 11, 4, 189, 318, 9, 4)], [(0, 11, 5, 215, 312, 10, 13)], [(0, 11, 6, 235, 313, 10, 12)], [(0, 11, 7, 255, 307, 15, 14)], [(0, 11, 8, 285, 312, 10, 13)], [(0, 11, 9, 309, 312, 9, 12)], [(0, 11, 10, 338, 312, 9, 12)], [(0, 11, 11, 367, 311, 18, 9)], [(0, 11, 12, 405, 312, 10, 13)], [(0, 11, 13, 437, 305, 8, 19)], [(0, 11, 14, 471, 316, 10, 4)]]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, True, False, False, False, True, False, False, False]\n",
      "Lyrics List: [[(0, 13, 1, 117, 343, 11, 14)], [(0, 13, 2, 137, 337, 20, 15)], [(0, 13, 3, 165, 342, 15, 14)], [(0, 13, 4, 190, 342, 8, 12)], [(0, 13, 5, 215, 342, 10, 11)], [(0, 13, 6, 234, 342, 9, 12)], [(0, 13, 7, 256, 341, 13, 10)], [(0, 13, 8, 282, 342, 15, 11)], [(0, 13, 9, 311, 341, 7, 12)], [(0, 13, 10, 338, 342, 11, 11)], [(0, 13, 11, 367, 340, 15, 19)], [(0, 13, 12, 405, 341, 9, 13)], [(0, 13, 13, 436, 341, 9, 12)], [(0, 13, 14, 473, 340, 7, 12)]]\n",
      "Lyrics Articulation Checks: [False, True, False, False, False, False, True, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (16, 19)\n",
      "Kann Swar List: [[], [], [], [], [(0, 16, 5, 213, 374, 12, 14)], [], [], [(0, 16, 9, 273, 376, 14, 12)], [], [], [], [(0, 16, 14, 399, 377, 6, 10)], [(0, 16, 15, 438, 377, 9, 10)], []]\n",
      "Swar List: [[(0, 18, 1, 116, 395, 15, 12)], [(0, 18, 2, 142, 400, 10, 4)], [(0, 18, 3, 162, 400, 11, 4)], [(0, 18, 4, 183, 395, 15, 12)], [(0, 18, 5, 215, 392, 14, 15)], [(0, 18, 7, 239, 391, 15, 16)], [(0, 18, 8, 263, 399, 10, 5)], [(0, 18, 10, 284, 395, 10, 12)], [(0, 18, 11, 308, 399, 9, 5)], [(0, 18, 12, 338, 391, 13, 19)], [(0, 18, 13, 372, 394, 9, 13)], [(0, 18, 14, 404, 394, 10, 13)], [(0, 18, 15, 437, 394, 10, 13)], [(0, 18, 17, 468, 399, 12, 4)]]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics List: [[(0, 19, 1, 115, 418, 15, 18)], [(0, 19, 2, 141, 423, 7, 13)], [(0, 19, 3, 165, 423, 8, 13)], [(0, 19, 4, 183, 424, 11, 11)], [(0, 19, 5, 214, 420, 9, 16)], [(0, 19, 7, 237, 423, 8, 12)], [(0, 19, 8, 263, 423, 7, 12)], [(0, 19, 10, 288, 424, 9, 14)], [(0, 19, 11, 310, 423, 8, 12)], [(0, 19, 12, 338, 423, 7, 12)], [(0, 19, 13, 367, 424, 13, 11)], [(0, 19, 14, 402, 417, 13, 22)], [(0, 19, 15, 432, 423, 13, 13)], [(0, 19, 16, 463, 423, 8, 13)], [(0, 19, 18, 478, 423, 5, 14)]]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (22, 24)\n",
      "Kann Swar List: [[(0, 22, 1, 114, 477, 12, 14)], [], [], [(0, 22, 6, 173, 478, 13, 12)], [(0, 22, 7, 194, 481, 9, 10)], [], [], [], [], [], [(0, 22, 14, 397, 477, 12, 13)], [], [], []]\n",
      "Swar List: [[(0, 23, 1, 115, 495, 14, 15)], [(0, 23, 2, 137, 502, 10, 5)], [(0, 23, 4, 158, 502, 10, 5)], [(0, 23, 6, 179, 494, 14, 16)], [(0, 23, 8, 204, 493, 14, 17)], [(0, 23, 9, 239, 494, 15, 16)], [(0, 23, 10, 275, 502, 9, 5)], [(0, 23, 11, 306, 493, 15, 17)], [(0, 23, 12, 336, 503, 9, 4)], [(0, 23, 13, 364, 494, 15, 16)], [(0, 23, 14, 403, 494, 15, 16)], [(0, 23, 15, 424, 493, 8, 18)], [(0, 23, 16, 442, 493, 15, 17)], [(0, 23, 17, 467, 502, 12, 4)]]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics List: [[(0, 24, 1, 115, 527, 13, 12)], [(0, 24, 3, 143, 526, 8, 13)], [(0, 24, 5, 164, 526, 8, 13)], [(0, 24, 6, 185, 527, 10, 12)], [(0, 24, 8, 209, 527, 9, 13)], [(0, 24, 9, 238, 527, 14, 12)], [(0, 24, 10, 274, 526, 8, 13)], [(0, 24, 11, 304, 527, 12, 12)], [(0, 24, 12, 337, 527, 8, 11)], [(0, 24, 13, 369, 527, 11, 15)], [(0, 24, 14, 403, 522, 9, 21)], [(0, 24, 15, 423, 526, 7, 13)], [(0, 24, 16, 449, 527, 10, 13)], [(0, 24, 17, 471, 526, 8, 12)]]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (26, 27)\n",
      "Kann Swar List: [[(0, 26, 1, 113, 560, 13, 13)], [], [], [], [], [], [], [], [], [], [], [(0, 26, 12, 397, 560, 12, 13)], [], [], []]\n",
      "Swar List: [[(0, 26, 1, 114, 577, 15, 16)], [(0, 26, 2, 140, 576, 10, 17)], [(0, 26, 3, 161, 585, 9, 4)], [(0, 26, 4, 179, 558, 8, 16)], [(0, 26, 5, 185, 576, 10, 18)], [(0, 26, 6, 208, 585, 10, 4)], [(0, 26, 7, 238, 585, 10, 5)], [(0, 26, 8, 274, 577, 10, 16)], [(0, 26, 9, 300, 561, 12, 31)], [(0, 26, 10, 337, 585, 10, 4)], [(0, 26, 11, 368, 585, 12, 4)], [(0, 26, 12, 402, 576, 15, 17)], [(0, 26, 13, 424, 575, 7, 18)], [(0, 26, 14, 441, 575, 15, 18)], [(0, 26, 15, 463, 576, 15, 17)]]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics List: [[(0, 27, 1, 114, 604, 13, 17)], [(0, 27, 2, 142, 609, 8, 11)], [(0, 27, 3, 160, 608, 8, 13)], [(0, 27, 5, 186, 610, 9, 12)], [(0, 27, 6, 210, 609, 8, 12)], [(0, 27, 7, 238, 609, 8, 11)], [(0, 27, 8, 274, 609, 10, 17)], [(0, 27, 9, 304, 609, 9, 13)], [(0, 27, 10, 341, 609, 8, 12)], [(0, 27, 11, 372, 609, 7, 12)], [(0, 27, 12, 402, 603, 10, 18)], [(0, 27, 13, 427, 609, 7, 12)], [(0, 27, 14, 446, 609, 9, 13)], [(0, 27, 15, 470, 608, 8, 14)]]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (29, 33)\n",
      "Kann Swar List: [[(0, 29, 1, 112, 645, 10, 10)], [], [], [], [], [(0, 29, 6, 233, 645, 10, 9)], [], [(0, 29, 8, 298, 646, 7, 10)], [(0, 29, 10, 332, 646, 9, 10)], [], [], [], [], []]\n",
      "Swar List: [[(0, 30, 1, 113, 663, 11, 12)], [(0, 30, 2, 133, 658, 21, 12)], [(0, 30, 3, 163, 657, 8, 19)], [(0, 30, 4, 181, 658, 15, 17)], [(0, 30, 5, 203, 658, 15, 17)], [(0, 30, 6, 239, 663, 10, 12)], [(0, 30, 7, 265, 658, 19, 14)], [(0, 30, 8, 303, 662, 10, 13)], [(0, 30, 10, 336, 662, 11, 14)], [(0, 30, 11, 370, 667, 10, 4)], [], [], [], []]\n",
      "Swar Articulation Checks: [False, True, False, False, False, False, True, False, False, False, False, False, False, False]\n",
      "Lyrics List: [[(0, 32, 1, 113, 688, 15, 16)], [(0, 32, 2, 138, 691, 13, 11)], [(0, 32, 3, 167, 692, 7, 12)], [(0, 32, 4, 185, 692, 12, 13)], [(0, 32, 5, 207, 692, 10, 12)], [(0, 32, 6, 238, 691, 15, 13)], [(0, 32, 7, 270, 690, 13, 11)], [(0, 32, 8, 304, 691, 8, 12)], [(0, 32, 10, 334, 691, 14, 13)], [(0, 32, 11, 371, 691, 8, 12)], [], [], [], []]\n",
      "Lyrics Articulation Checks: [False, True, False, False, False, False, True, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# we just appended lyrics from left to right, hoping that when user will correct the swar row\n",
    "# then it will automattically be mapped perfectly with the swar\n",
    "\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the path to the folder containing the images\n",
    "image_folder_path = 'Analysis/bilawal_dhamaar'\n",
    "\n",
    "# Define the section-wise row numbers\n",
    "articulation_rows = [12, 14, 31, 33]\n",
    "kann_swar_rows = [4, 10, 16, 22, 29]\n",
    "swar_rows = [5, 7, 11, 18, 23, 26, 30]\n",
    "lyrics_rows = [6, 8, 13, 19, 24, 27, 32]\n",
    "\n",
    "# Define the subgroup ranges\n",
    "subgroup_ranges = [\n",
    "    (4, 7),\n",
    "    (7, 8),\n",
    "    (10, 14),\n",
    "    (16, 19),\n",
    "    (22, 24),\n",
    "    (26, 27),\n",
    "    (29, 33)\n",
    "]\n",
    "\n",
    "# Define the beat count (size of the lists)\n",
    "beat_count = 14\n",
    "\n",
    "# Function to extract information from the image filename\n",
    "def extract_info_from_filename(filename):\n",
    "    pattern = r'(\\d+)_row(\\d+)(?:_col(\\d+))?_x(\\d+)_y(\\d+)_w(\\d+)_h(\\d+)'\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        page_num = int(match.group(1))\n",
    "        row_num = int(match.group(2))\n",
    "        col_num = int(match.group(3)) if match.group(3) else None\n",
    "        x = int(match.group(4))\n",
    "        y = int(match.group(5))\n",
    "        width = int(match.group(6))\n",
    "        height = int(match.group(7))\n",
    "        return page_num, row_num, col_num, x, y, width, height\n",
    "    return None\n",
    "\n",
    "# Load all image filenames and extract their information\n",
    "image_files = os.listdir(image_folder_path)\n",
    "image_info = [extract_info_from_filename(f) for f in image_files]\n",
    "image_info = [info for info in image_info if info is not None]\n",
    "\n",
    "# Organize images by row and column\n",
    "row_col_images = defaultdict(lambda: defaultdict(list))\n",
    "for info in image_info:\n",
    "    page_num, row_num, col_num, x, y, width, height = info\n",
    "    row_col_images[row_num][col_num].append(info)\n",
    "\n",
    "# Function to pad lists to match the beat count\n",
    "def pad_lists(lists, size):\n",
    "    if len(lists) < size:\n",
    "        padding = [[] for _ in range(size - len(lists))]\n",
    "        return padding + lists\n",
    "    return lists\n",
    "\n",
    "# Function to process a subgroup and create the lists of lists\n",
    "def process_subgroup(subgroup_range, is_first_subgroup):\n",
    "    start_row, end_row = subgroup_range\n",
    "    \n",
    "    # Find the swar row in this subgroup\n",
    "    swar_row = None\n",
    "    for row in swar_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            swar_row = row\n",
    "            break\n",
    "    \n",
    "    if not swar_row:\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    # Find the kann swar row in this subgroup\n",
    "    kann_swar_row = None\n",
    "    for row in kann_swar_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            kann_swar_row = row\n",
    "            break\n",
    "    \n",
    "    # Find the articulation rows in this subgroup\n",
    "    articulation_rows_in_subgroup = [row for row in articulation_rows if start_row <= row <= end_row]\n",
    "    \n",
    "    # Find the lyrics row in this subgroup\n",
    "    lyrics_row = None\n",
    "    for row in lyrics_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            lyrics_row = row\n",
    "            break\n",
    "    \n",
    "    # Get the swar images and their column numbers\n",
    "    swar_images = row_col_images[swar_row]\n",
    "    swar_cols = sorted(swar_images.keys())\n",
    "    \n",
    "    # Get the kann swar images and their column numbers (if kann swar row exists)\n",
    "    kann_swar_images = row_col_images[kann_swar_row] if kann_swar_row else {}\n",
    "    kann_swar_cols = sorted(kann_swar_images.keys())\n",
    "    \n",
    "    # Get the lyrics images (if lyrics row exists)\n",
    "    lyrics_images = row_col_images[lyrics_row] if lyrics_row else {}\n",
    "    lyrics_cols = sorted(lyrics_images.keys()) if lyrics_row else []\n",
    "    \n",
    "    # Create the lists of lists\n",
    "    swar_list = []\n",
    "    kann_swar_list = []\n",
    "    swar_articulation_checks = [False] * len(swar_cols)\n",
    "    lyrics_articulation_checks = [False] * len(lyrics_cols)\n",
    "    lyrics_list = []\n",
    "    \n",
    "    # Case 1: If there is an explicit kann swar row\n",
    "    if kann_swar_row:\n",
    "        swar_index = 0\n",
    "        kann_swar_index = 0\n",
    "        \n",
    "        while swar_index < len(swar_cols) or kann_swar_index < len(kann_swar_cols):\n",
    "            swar_col = swar_cols[swar_index] if swar_index < len(swar_cols) else None\n",
    "            kann_swar_col = kann_swar_cols[kann_swar_index] if kann_swar_index < len(kann_swar_cols) else None\n",
    "            \n",
    "            # If both columns exist and match\n",
    "            if swar_col is not None and kann_swar_col is not None and swar_col == kann_swar_col:\n",
    "                swar_list.append(swar_images[swar_col])\n",
    "                kann_swar_list.append(kann_swar_images[kann_swar_col])\n",
    "                swar_index += 1\n",
    "                kann_swar_index += 1\n",
    "            # If swar column exists but kann swar column doesn't match or is missing\n",
    "            elif swar_col is not None and (kann_swar_col is None or swar_col < kann_swar_col):\n",
    "                swar_list.append(swar_images[swar_col])\n",
    "                kann_swar_list.append([])\n",
    "                swar_index += 1\n",
    "            # If kann swar column exists but swar column doesn't match or is missing\n",
    "            elif kann_swar_col is not None and (swar_col is None or kann_swar_col < swar_col):\n",
    "                # Assign the kann swar to the next available swar column\n",
    "                if swar_index < len(swar_cols):\n",
    "                    swar_list.append(swar_images[swar_cols[swar_index]])\n",
    "                    kann_swar_list.append(kann_swar_images[kann_swar_col])\n",
    "                    swar_index += 1\n",
    "                    kann_swar_index += 1\n",
    "                else:\n",
    "                    # If no more swar columns are available, append an empty list\n",
    "                    swar_list.append([])\n",
    "                    kann_swar_list.append(kann_swar_images[kann_swar_col])\n",
    "                    kann_swar_index += 1\n",
    "    \n",
    "    # Case 2: If there is no explicit kann swar row, check for hidden kann swars in the swar row\n",
    "    else:\n",
    "        for col in swar_cols:\n",
    "            images_in_col = swar_images[col]\n",
    "            if len(images_in_col) == 1:\n",
    "                # Only one image in this column, so no hidden kann swar\n",
    "                swar_list.append(images_in_col)\n",
    "                kann_swar_list.append([])\n",
    "            else:\n",
    "                # Multiple images in the same column, so identify hidden kann swars\n",
    "                # Sort images by y-value (lower y-value is kann swar)\n",
    "                sorted_images = sorted(images_in_col, key=lambda x: x[4])  # Sort by y-value\n",
    "                kann_swar_list.append([sorted_images[0]])  # Lower y-value is kann swar\n",
    "                swar_list.append([sorted_images[1]])  # Higher y-value is swar\n",
    "    \n",
    "    # Handle articulation rows\n",
    "    for articulation_row in articulation_rows_in_subgroup:\n",
    "        # Find the row just before the articulation row\n",
    "        prev_row = articulation_row - 1\n",
    "        if prev_row in swar_rows:\n",
    "            # Swar articulation\n",
    "            articulation_images = row_col_images[articulation_row]\n",
    "            articulation_cols = sorted(articulation_images.keys())\n",
    "            for i, col in enumerate(swar_cols):\n",
    "                if col in articulation_cols:\n",
    "                    swar_articulation_checks[i] = True\n",
    "        elif prev_row in lyrics_rows:\n",
    "            # Lyrics articulation\n",
    "            articulation_images = row_col_images[articulation_row]\n",
    "            articulation_cols = sorted(articulation_images.keys())\n",
    "            for i, col in enumerate(swar_cols):\n",
    "                if col in articulation_cols:\n",
    "                    lyrics_articulation_checks[i] = True\n",
    "    \n",
    "    # Handle lyrics row (append images one by one without comparing column numbers)\n",
    "    if lyrics_row:\n",
    "        # Get all lyrics images in order\n",
    "        lyrics_cols = sorted(lyrics_images.keys())\n",
    "        for col in lyrics_cols:\n",
    "            lyrics_list.append(lyrics_images[col])\n",
    "    else:\n",
    "        lyrics_list = [[] for _ in range(len(swar_cols))]\n",
    "    \n",
    "    # Pad lists to match the beat count\n",
    "    if is_first_subgroup:\n",
    "        swar_list = pad_lists(swar_list, beat_count)\n",
    "        kann_swar_list = pad_lists(kann_swar_list, beat_count)\n",
    "        swar_articulation_checks = pad_lists(swar_articulation_checks, beat_count)\n",
    "        lyrics_articulation_checks = pad_lists(lyrics_articulation_checks, beat_count)\n",
    "        lyrics_list = pad_lists(lyrics_list, beat_count)\n",
    "    else:\n",
    "        if len(swar_list) < beat_count:\n",
    "            swar_list += [[] for _ in range(beat_count - len(swar_list))]\n",
    "        if len(kann_swar_list) < beat_count:\n",
    "            kann_swar_list += [[] for _ in range(beat_count - len(kann_swar_list))]\n",
    "        if len(swar_articulation_checks) < beat_count:\n",
    "            swar_articulation_checks += [False for _ in range(beat_count - len(swar_articulation_checks))]\n",
    "        if len(lyrics_articulation_checks) < beat_count:\n",
    "            lyrics_articulation_checks += [False for _ in range(beat_count - len(lyrics_articulation_checks))]\n",
    "        if len(lyrics_list) < beat_count:\n",
    "            lyrics_list += [[] for _ in range(beat_count - len(lyrics_list))]\n",
    "    \n",
    "    return swar_list, kann_swar_list, swar_articulation_checks, lyrics_articulation_checks, lyrics_list\n",
    "\n",
    "# Process each subgroup and store the results\n",
    "subgroup_results = {}\n",
    "for i, subgroup_range in enumerate(subgroup_ranges):\n",
    "    is_first_subgroup = (i == 0)\n",
    "    swar_list, kann_swar_list, swar_articulation_checks, lyrics_articulation_checks, lyrics_list = process_subgroup(subgroup_range, is_first_subgroup)\n",
    "    if swar_list and kann_swar_list:\n",
    "        subgroup_results[subgroup_range] = {\n",
    "            'swar_list': swar_list,\n",
    "            'kann_swar_list': kann_swar_list,\n",
    "            'swar_articulation_checks': swar_articulation_checks,\n",
    "            'lyrics_articulation_checks': lyrics_articulation_checks,\n",
    "            'lyrics_list': lyrics_list\n",
    "        }\n",
    "\n",
    "# Print the results for each subgroup\n",
    "for subgroup_range, results in subgroup_results.items():\n",
    "    print(f\"Subgroup Range: {subgroup_range}\")\n",
    "    print(f\"Kann Swar List: {results['kann_swar_list']}\")\n",
    "    print(f\"Swar List: {results['swar_list']}\")\n",
    "    print(f\"Swar Articulation Checks: {results['swar_articulation_checks']}\")\n",
    "    print(f\"Lyrics List: {results['lyrics_list']}\")\n",
    "    print(f\"Lyrics Articulation Checks: {results['lyrics_articulation_checks']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroup Range: (4, 7)\n",
      "Kann Swar List: [[], [], [], [], [], [], [], [], [], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row4_col12_x400_y145_w7_h10.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row4_col13_x434_y145_w9_h10.png'], []]\n",
      "Swar List: [[], [], [], [], [], [], [], [], [], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row5_col12_x406_y162_w10_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row5_col13_x439_y162_w10_h14.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row5_col14_x472_y166_w9_h5.png']]\n",
      "Swar Articulation Checks: [[], [], [], [], [], [], [], [], [], [], [], False, False, False]\n",
      "Lyrics List: [[], [], [], [], [], [], [], [], [], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row6_col12_x405_y188_w11_h15.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row6_col13_x436_y188_w14_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row6_col14_x473_y187_w8_h12.png']]\n",
      "Lyrics Articulation Checks: [[], [], [], [], [], [], [], [], [], [], [], False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (7, 8)\n",
      "Kann Swar List: [[], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col3_x156_y211_w14_h12.png'], [], [], [], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col9_x303_y213_w8_h8.png'], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col12_x401_y211_w13_h10.png'], [], []]\n",
      "Swar List: [['Analysis\\\\bilawal_dhamaar\\\\0_row7_col1_x118_y231_w9_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col2_x139_y236_w9_h4.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col3_x159_y227_w14_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col4_x181_y214_w17_h29.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col5_x215_y227_w15_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col6_x240_y235_w10_h5.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col7_x260_y235_w11_h5.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col8_x281_y226_w14_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col9_x304_y226_w14_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col10_x339_y227_w15_h15.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col11_x376_y235_w9_h4.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col12_x405_y225_w7_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col13_x434_y226_w15_h15.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col14_x470_y234_w10_h4.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics List: [['Analysis\\\\bilawal_dhamaar\\\\0_row8_col1_x118_y256_w12_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col2_x141_y260_w8_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col3_x163_y260_w9_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col4_x184_y260_w11_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col5_x215_y259_w13_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col6_x242_y259_w8_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col7_x263_y259_w8_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col8_x285_y258_w10_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col9_x310_y259_w9_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col10_x339_y252_w14_h19.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col11_x375_y258_w8_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col12_x406_y258_w8_h14.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col13_x441_y258_w10_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col14_x473_y257_w7_h13.png']]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (10, 14)\n",
      "Kann Swar List: [['Analysis\\\\bilawal_dhamaar\\\\0_row10_col1_x116_y294_w14_h13.png'], [], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row10_col5_x213_y296_w6_h9.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row10_col6_x232_y296_w7_h9.png'], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row10_col8_x278_y295_w9_h9.png'], [], [], [], [], [], []]\n",
      "Swar List: [['Analysis\\\\bilawal_dhamaar\\\\0_row11_col1_x117_y314_w10_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col2_x137_y309_w22_h23.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col3_x169_y313_w8_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col4_x189_y318_w9_h4.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col5_x215_y312_w10_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col6_x235_y313_w10_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col7_x255_y307_w15_h14.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col8_x285_y312_w10_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col9_x309_y312_w9_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col10_x338_y312_w9_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col11_x367_y311_w18_h9.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col12_x405_y312_w10_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col13_x437_y305_w8_h19.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col14_x471_y316_w10_h4.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, True, False, False, False, True, False, False, False]\n",
      "Lyrics List: [['Analysis\\\\bilawal_dhamaar\\\\0_row13_col1_x117_y343_w11_h14.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col2_x137_y337_w20_h15.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col3_x165_y342_w15_h14.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col4_x190_y342_w8_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col5_x215_y342_w10_h11.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col6_x234_y342_w9_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col7_x256_y341_w13_h10.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col8_x282_y342_w15_h11.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col9_x311_y341_w7_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col10_x338_y342_w11_h11.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col11_x367_y340_w15_h19.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col12_x405_y341_w9_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col13_x436_y341_w9_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col14_x473_y340_w7_h12.png']]\n",
      "Lyrics Articulation Checks: [False, True, False, False, False, False, True, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (16, 19)\n",
      "Kann Swar List: [[], [], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row16_col5_x213_y374_w12_h14.png'], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row16_col9_x273_y376_w14_h12.png'], [], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row16_col14_x399_y377_w6_h10.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row16_col15_x438_y377_w9_h10.png'], []]\n",
      "Swar List: [['Analysis\\\\bilawal_dhamaar\\\\0_row18_col1_x116_y395_w15_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col2_x142_y400_w10_h4.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col3_x162_y400_w11_h4.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col4_x183_y395_w15_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col5_x215_y392_w14_h15.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col7_x239_y391_w15_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col8_x263_y399_w10_h5.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col10_x284_y395_w10_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col11_x308_y399_w9_h5.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col12_x338_y391_w13_h19.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col13_x372_y394_w9_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col14_x404_y394_w10_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col15_x437_y394_w10_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col17_x468_y399_w12_h4.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics List: [['Analysis\\\\bilawal_dhamaar\\\\0_row19_col1_x115_y418_w15_h18.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col2_x141_y423_w7_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col3_x165_y423_w8_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col4_x183_y424_w11_h11.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col5_x214_y420_w9_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col7_x237_y423_w8_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col8_x263_y423_w7_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col10_x288_y424_w9_h14.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col11_x310_y423_w8_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col12_x338_y423_w7_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col13_x367_y424_w13_h11.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col14_x402_y417_w13_h22.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col15_x432_y423_w13_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col16_x463_y423_w8_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col18_x478_y423_w5_h14.png']]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (22, 24)\n",
      "Kann Swar List: [['Analysis\\\\bilawal_dhamaar\\\\0_row22_col1_x114_y477_w12_h14.png'], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row22_col6_x173_y478_w13_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row22_col7_x194_y481_w9_h10.png'], [], [], [], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row22_col14_x397_y477_w12_h13.png'], [], [], []]\n",
      "Swar List: [['Analysis\\\\bilawal_dhamaar\\\\0_row23_col1_x115_y495_w14_h15.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col2_x137_y502_w10_h5.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col4_x158_y502_w10_h5.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col6_x179_y494_w14_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col8_x204_y493_w14_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col9_x239_y494_w15_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col10_x275_y502_w9_h5.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col11_x306_y493_w15_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col12_x336_y503_w9_h4.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col13_x364_y494_w15_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col14_x403_y494_w15_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col15_x424_y493_w8_h18.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col16_x442_y493_w15_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col17_x467_y502_w12_h4.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics List: [['Analysis\\\\bilawal_dhamaar\\\\0_row24_col1_x115_y527_w13_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col3_x143_y526_w8_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col5_x164_y526_w8_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col6_x185_y527_w10_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col8_x209_y527_w9_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col9_x238_y527_w14_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col10_x274_y526_w8_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col11_x304_y527_w12_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col12_x337_y527_w8_h11.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col13_x369_y527_w11_h15.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col14_x403_y522_w9_h21.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col15_x423_y526_w7_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col16_x449_y527_w10_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col17_x471_y526_w8_h12.png']]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (26, 27)\n",
      "Kann Swar List: [['Analysis\\\\bilawal_dhamaar\\\\0_row26_col1_x113_y560_w13_h13.png'], [], [], [], [], [], [], [], [], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col12_x397_y560_w12_h13.png'], [], [], []]\n",
      "Swar List: [['Analysis\\\\bilawal_dhamaar\\\\0_row26_col1_x114_y577_w15_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col2_x140_y576_w10_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col3_x161_y585_w9_h4.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col4_x179_y558_w8_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col5_x185_y576_w10_h18.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col6_x208_y585_w10_h4.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col7_x238_y585_w10_h5.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col8_x274_y577_w10_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col9_x300_y561_w12_h31.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col10_x337_y585_w10_h4.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col11_x368_y585_w12_h4.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col12_x402_y576_w15_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col13_x424_y575_w7_h18.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col14_x441_y575_w15_h18.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col15_x463_y576_w15_h17.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics List: [['Analysis\\\\bilawal_dhamaar\\\\0_row27_col1_x114_y604_w13_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col2_x142_y609_w8_h11.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col3_x160_y608_w8_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col5_x186_y610_w9_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col6_x210_y609_w8_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col7_x238_y609_w8_h11.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col8_x274_y609_w10_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col9_x304_y609_w9_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col10_x341_y609_w8_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col11_x372_y609_w7_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col12_x402_y603_w10_h18.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col13_x427_y609_w7_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col14_x446_y609_w9_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col15_x470_y608_w8_h14.png']]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (29, 33)\n",
      "Kann Swar List: [['Analysis\\\\bilawal_dhamaar\\\\0_row29_col1_x112_y645_w10_h10.png'], [], [], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row29_col6_x233_y645_w10_h9.png'], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row29_col8_x298_y646_w7_h10.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row29_col10_x332_y646_w9_h10.png'], [], [], [], [], []]\n",
      "Swar List: [['Analysis\\\\bilawal_dhamaar\\\\0_row30_col1_x113_y663_w11_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row30_col2_x133_y658_w21_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row30_col3_x163_y657_w8_h19.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row30_col4_x181_y658_w15_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row30_col5_x203_y658_w15_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row30_col6_x239_y663_w10_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row30_col7_x265_y658_w19_h14.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row30_col8_x303_y662_w10_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row30_col10_x336_y662_w11_h14.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row30_col11_x370_y667_w10_h4.png'], [], [], [], []]\n",
      "Swar Articulation Checks: [False, True, False, False, False, False, True, False, False, False, False, False, False, False]\n",
      "Lyrics List: [['Analysis\\\\bilawal_dhamaar\\\\0_row32_col1_x113_y688_w15_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row32_col2_x138_y691_w13_h11.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row32_col3_x167_y692_w7_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row32_col4_x185_y692_w12_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row32_col5_x207_y692_w10_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row32_col6_x238_y691_w15_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row32_col7_x270_y690_w13_h11.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row32_col8_x304_y691_w8_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row32_col10_x334_y691_w14_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row32_col11_x371_y691_w8_h12.png'], [], [], [], []]\n",
      "Lyrics Articulation Checks: [False, True, False, False, False, False, True, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# same code as above but storing image paths in lists for direct access\n",
    "\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the path to the folder containing the images\n",
    "image_folder_path = 'Analysis/bilawal_dhamaar'\n",
    "\n",
    "# Define the section-wise row numbers\n",
    "articulation_rows = [12, 14, 31, 33]\n",
    "kann_swar_rows = [4, 10, 16, 22, 29]\n",
    "swar_rows = [5, 7, 11, 18, 23, 26, 30]\n",
    "lyrics_rows = [6, 8, 13, 19, 24, 27, 32]\n",
    "\n",
    "# Define the subgroup ranges\n",
    "subgroup_ranges = [\n",
    "    (4, 7),\n",
    "    (7, 8),\n",
    "    (10, 14),\n",
    "    (16, 19),\n",
    "    (22, 24),\n",
    "    (26, 27),\n",
    "    (29, 33)\n",
    "]\n",
    "\n",
    "# Define the beat count (size of the lists)\n",
    "beat_count = 14\n",
    "\n",
    "# Function to extract information from the image filename\n",
    "def extract_info_from_filename(filename):\n",
    "    pattern = r'(\\d+)_row(\\d+)(?:_col(\\d+))?_x(\\d+)_y(\\d+)_w(\\d+)_h(\\d+)'\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        page_num = int(match.group(1))\n",
    "        row_num = int(match.group(2))\n",
    "        col_num = int(match.group(3)) if match.group(3) else None\n",
    "        x = int(match.group(4))\n",
    "        y = int(match.group(5))\n",
    "        width = int(match.group(6))\n",
    "        height = int(match.group(7))\n",
    "        # Use os.path.join to handle path separators correctly\n",
    "        image_path = os.path.normpath(os.path.join(image_folder_path, filename))\n",
    "        return page_num, row_num, col_num, x, y, width, height, image_path\n",
    "    return None\n",
    "\n",
    "# Load all image filenames and extract their information\n",
    "image_files = os.listdir(image_folder_path)\n",
    "image_info = [extract_info_from_filename(f) for f in image_files]\n",
    "image_info = [info for info in image_info if info is not None]\n",
    "\n",
    "# Organize images by row and column\n",
    "row_col_images = defaultdict(lambda: defaultdict(list))\n",
    "for info in image_info:\n",
    "    page_num, row_num, col_num, x, y, width, height, image_path = info\n",
    "    row_col_images[row_num][col_num].append((x, y, width, height, image_path))\n",
    "\n",
    "# Function to pad lists to match the beat count\n",
    "def pad_lists(lists, size):\n",
    "    if len(lists) < size:\n",
    "        padding = [[] for _ in range(size - len(lists))]\n",
    "        return padding + lists\n",
    "    return lists\n",
    "\n",
    "# Function to process a subgroup and create the lists of lists\n",
    "def process_subgroup(subgroup_range, is_first_subgroup):\n",
    "    start_row, end_row = subgroup_range\n",
    "    \n",
    "    # Find the swar row in this subgroup\n",
    "    swar_row = None\n",
    "    for row in swar_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            swar_row = row\n",
    "            break\n",
    "    \n",
    "    if not swar_row:\n",
    "        return None, None, None, None, None\n",
    "    \n",
    "    # Find the kann swar row in this subgroup\n",
    "    kann_swar_row = None\n",
    "    for row in kann_swar_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            kann_swar_row = row\n",
    "            break\n",
    "    \n",
    "    # Find the articulation rows in this subgroup\n",
    "    articulation_rows_in_subgroup = [row for row in articulation_rows if start_row <= row <= end_row]\n",
    "    \n",
    "    # Find the lyrics row in this subgroup\n",
    "    lyrics_row = None\n",
    "    for row in lyrics_rows:\n",
    "        if start_row <= row <= end_row:\n",
    "            lyrics_row = row\n",
    "            break\n",
    "    \n",
    "    # Get the swar images and their column numbers\n",
    "    swar_images = row_col_images[swar_row]\n",
    "    swar_cols = sorted(swar_images.keys())\n",
    "    \n",
    "    # Get the kann swar images and their column numbers (if kann swar row exists)\n",
    "    kann_swar_images = row_col_images[kann_swar_row] if kann_swar_row else {}\n",
    "    kann_swar_cols = sorted(kann_swar_images.keys())\n",
    "    \n",
    "    # Get the lyrics images (if lyrics row exists)\n",
    "    lyrics_images = row_col_images[lyrics_row] if lyrics_row else {}\n",
    "    lyrics_cols = sorted(lyrics_images.keys()) if lyrics_row else []\n",
    "    \n",
    "    # Create the lists of lists\n",
    "    swar_list = []\n",
    "    kann_swar_list = []\n",
    "    swar_articulation_checks = [False] * len(swar_cols)\n",
    "    lyrics_articulation_checks = [False] * len(lyrics_cols)\n",
    "    lyrics_list = []\n",
    "    \n",
    "    # Case 1: If there is an explicit kann swar row\n",
    "    if kann_swar_row:\n",
    "        swar_index = 0\n",
    "        kann_swar_index = 0\n",
    "        \n",
    "        while swar_index < len(swar_cols) or kann_swar_index < len(kann_swar_cols):\n",
    "            swar_col = swar_cols[swar_index] if swar_index < len(swar_cols) else None\n",
    "            kann_swar_col = kann_swar_cols[kann_swar_index] if kann_swar_index < len(kann_swar_cols) else None\n",
    "            \n",
    "            # If both columns exist and match\n",
    "            if swar_col is not None and kann_swar_col is not None and swar_col == kann_swar_col:\n",
    "                swar_list.append([x[4] for x in swar_images[swar_col]])  # Store image paths\n",
    "                kann_swar_list.append([x[4] for x in kann_swar_images[kann_swar_col]])  # Store image paths\n",
    "                swar_index += 1\n",
    "                kann_swar_index += 1\n",
    "            # If swar column exists but kann swar column doesn't match or is missing\n",
    "            elif swar_col is not None and (kann_swar_col is None or swar_col < kann_swar_col):\n",
    "                swar_list.append([x[4] for x in swar_images[swar_col]])  # Store image paths\n",
    "                kann_swar_list.append([])\n",
    "                swar_index += 1\n",
    "            # If kann swar column exists but swar column doesn't match or is missing\n",
    "            elif kann_swar_col is not None and (swar_col is None or kann_swar_col < swar_col):\n",
    "                # Assign the kann swar to the next available swar column\n",
    "                if swar_index < len(swar_cols):\n",
    "                    swar_list.append([x[4] for x in swar_images[swar_cols[swar_index]]])  # Store image paths\n",
    "                    kann_swar_list.append([x[4] for x in kann_swar_images[kann_swar_col]])  # Store image paths\n",
    "                    swar_index += 1\n",
    "                    kann_swar_index += 1\n",
    "                else:\n",
    "                    # If no more swar columns are available, append an empty list\n",
    "                    swar_list.append([])\n",
    "                    kann_swar_list.append([x[4] for x in kann_swar_images[kann_swar_col]])  # Store image paths\n",
    "                    kann_swar_index += 1\n",
    "    \n",
    "    # Case 2: If there is no explicit kann swar row, check for hidden kann swars in the swar row\n",
    "    else:\n",
    "        for col in swar_cols:\n",
    "            images_in_col = swar_images[col]\n",
    "            if len(images_in_col) == 1:\n",
    "                # Only one image in this column, so no hidden kann swar\n",
    "                swar_list.append([images_in_col[0][4]])  # Store image path\n",
    "                kann_swar_list.append([])\n",
    "            else:\n",
    "                # Multiple images in the same column, so identify hidden kann swars\n",
    "                # Sort images by y-value (lower y-value is kann swar)\n",
    "                sorted_images = sorted(images_in_col, key=lambda x: x[1])  # Sort by y-value\n",
    "                kann_swar_list.append([sorted_images[0][4]])  # Lower y-value is kann swar (store image path)\n",
    "                swar_list.append([sorted_images[1][4]])  # Higher y-value is swar (store image path)\n",
    "    \n",
    "    # Handle articulation rows\n",
    "    for articulation_row in articulation_rows_in_subgroup:\n",
    "        # Find the row just before the articulation row\n",
    "        prev_row = articulation_row - 1\n",
    "        if prev_row in swar_rows:\n",
    "            # Swar articulation\n",
    "            articulation_images = row_col_images[articulation_row]\n",
    "            articulation_cols = sorted(articulation_images.keys())\n",
    "            for i, col in enumerate(swar_cols):\n",
    "                if col in articulation_cols:\n",
    "                    swar_articulation_checks[i] = True\n",
    "        elif prev_row in lyrics_rows:\n",
    "            # Lyrics articulation\n",
    "            articulation_images = row_col_images[articulation_row]\n",
    "            articulation_cols = sorted(articulation_images.keys())\n",
    "            for i, col in enumerate(swar_cols):\n",
    "                if col in articulation_cols:\n",
    "                    lyrics_articulation_checks[i] = True\n",
    "    \n",
    "    # Handle lyrics row (append images one by one without comparing column numbers)\n",
    "    if lyrics_row:\n",
    "        # Get all lyrics images in order\n",
    "        lyrics_cols = sorted(lyrics_images.keys())\n",
    "        for col in lyrics_cols:\n",
    "            lyrics_list.append([x[4] for x in lyrics_images[col]])  # Store image paths\n",
    "    else:\n",
    "        lyrics_list = [[] for _ in range(len(swar_cols))]\n",
    "    \n",
    "    # Pad lists to match the beat count\n",
    "    if is_first_subgroup:\n",
    "        swar_list = pad_lists(swar_list, beat_count)\n",
    "        kann_swar_list = pad_lists(kann_swar_list, beat_count)\n",
    "        swar_articulation_checks = pad_lists(swar_articulation_checks, beat_count)\n",
    "        lyrics_articulation_checks = pad_lists(lyrics_articulation_checks, beat_count)\n",
    "        lyrics_list = pad_lists(lyrics_list, beat_count)\n",
    "    else:\n",
    "        if len(swar_list) < beat_count:\n",
    "            swar_list += [[] for _ in range(beat_count - len(swar_list))]\n",
    "        if len(kann_swar_list) < beat_count:\n",
    "            kann_swar_list += [[] for _ in range(beat_count - len(kann_swar_list))]\n",
    "        if len(swar_articulation_checks) < beat_count:\n",
    "            swar_articulation_checks += [False for _ in range(beat_count - len(swar_articulation_checks))]\n",
    "        if len(lyrics_articulation_checks) < beat_count:\n",
    "            lyrics_articulation_checks += [False for _ in range(beat_count - len(lyrics_articulation_checks))]\n",
    "        if len(lyrics_list) < beat_count:\n",
    "            lyrics_list += [[] for _ in range(beat_count - len(lyrics_list))]\n",
    "    \n",
    "    return swar_list, kann_swar_list, swar_articulation_checks, lyrics_articulation_checks, lyrics_list\n",
    "\n",
    "# Process each subgroup and store the results\n",
    "subgroup_results = {}\n",
    "for i, subgroup_range in enumerate(subgroup_ranges):\n",
    "    is_first_subgroup = (i == 0)\n",
    "    swar_list, kann_swar_list, swar_articulation_checks, lyrics_articulation_checks, lyrics_list = process_subgroup(subgroup_range, is_first_subgroup)\n",
    "    if swar_list and kann_swar_list:\n",
    "        subgroup_results[subgroup_range] = {\n",
    "            'swar_list': swar_list,\n",
    "            'kann_swar_list': kann_swar_list,\n",
    "            'swar_articulation_checks': swar_articulation_checks,\n",
    "            'lyrics_articulation_checks': lyrics_articulation_checks,\n",
    "            'lyrics_list': lyrics_list\n",
    "        }\n",
    "\n",
    "# Print the results for each subgroup\n",
    "for subgroup_range, results in subgroup_results.items():\n",
    "    print(f\"Subgroup Range: {subgroup_range}\")\n",
    "    print(f\"Kann Swar List: {results['kann_swar_list']}\")\n",
    "    print(f\"Swar List: {results['swar_list']}\")\n",
    "    print(f\"Swar Articulation Checks: {results['swar_articulation_checks']}\")\n",
    "    print(f\"Lyrics List: {results['lyrics_list']}\")\n",
    "    print(f\"Lyrics Articulation Checks: {results['lyrics_articulation_checks']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subgroup Range: (4, 7)\n",
      "Kann Swar List: [[], [], [], [], [], [], [], [], [], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row4_col12_x400_y145_w7_h10.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row4_col13_x434_y145_w9_h10.png'], []]\n",
      "Swar List: [[], [], [], [], [], [], [], [], [], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row5_col12_x406_y162_w10_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row5_col13_x439_y162_w10_h14.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row5_col14_x472_y166_w9_h5.png']]\n",
      "Lyrics List: [[], [], [], [], [], [], [], [], [], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row6_col12_x405_y188_w11_h15.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row6_col13_x436_y188_w14_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row6_col14_x473_y187_w8_h12.png']]\n",
      "Swar Articulation Checks: [[], [], [], [], [], [], [], [], [], [], [], False, False, False]\n",
      "Lyrics Articulation Checks: [[], [], [], [], [], [], [], [], [], [], [], False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (7, 8)\n",
      "Kann Swar List: [[], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col3_x156_y211_w14_h12.png'], [], [], [], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col9_x303_y213_w8_h8.png'], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col12_x401_y211_w13_h10.png'], [], []]\n",
      "Swar List: [['Analysis\\\\bilawal_dhamaar\\\\0_row7_col1_x118_y231_w9_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col2_x139_y236_w9_h4.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col3_x159_y227_w14_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col4_x181_y214_w17_h29.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col5_x215_y227_w15_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col6_x240_y235_w10_h5.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col7_x260_y235_w11_h5.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col8_x281_y226_w14_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col9_x304_y226_w14_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col10_x339_y227_w15_h15.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col11_x376_y235_w9_h4.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col12_x405_y225_w7_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col13_x434_y226_w15_h15.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row7_col14_x470_y234_w10_h4.png']]\n",
      "Lyrics List: [['Analysis\\\\bilawal_dhamaar\\\\0_row8_col1_x118_y256_w12_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col2_x141_y260_w8_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col3_x163_y260_w9_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col4_x184_y260_w11_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col5_x215_y259_w13_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col6_x242_y259_w8_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col7_x263_y259_w8_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col8_x285_y258_w10_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col9_x310_y259_w9_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col10_x339_y252_w14_h19.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col11_x375_y258_w8_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col12_x406_y258_w8_h14.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col13_x441_y258_w10_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row8_col14_x473_y257_w7_h13.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (10, 14)\n",
      "Kann Swar List: [['Analysis\\\\bilawal_dhamaar\\\\0_row10_col1_x116_y294_w14_h13.png'], [], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row10_col5_x213_y296_w6_h9.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row10_col6_x232_y296_w7_h9.png'], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row10_col8_x278_y295_w9_h9.png'], [], [], [], [], [], []]\n",
      "Swar List: [['Analysis\\\\bilawal_dhamaar\\\\0_row11_col1_x117_y314_w10_h12.png'], ['Analysis\\\\bilawal_dhamaar_segmented\\\\0_row11_col2_x137_y309_w22_h23_seg1.png', 'Analysis\\\\bilawal_dhamaar_segmented\\\\0_row11_col2_x137_y309_w22_h23_seg2.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col3_x169_y313_w8_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col4_x189_y318_w9_h4.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col5_x215_y312_w10_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col6_x235_y313_w10_h12.png'], ['Analysis\\\\bilawal_dhamaar_segmented\\\\0_row11_col7_x255_y307_w15_h14_seg1.png', 'Analysis\\\\bilawal_dhamaar_segmented\\\\0_row11_col7_x255_y307_w15_h14_seg2.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col8_x285_y312_w10_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col9_x309_y312_w9_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col10_x338_y312_w9_h12.png'], ['Analysis\\\\bilawal_dhamaar_segmented\\\\0_row11_col11_x367_y311_w18_h9_seg1.png', 'Analysis\\\\bilawal_dhamaar_segmented\\\\0_row11_col11_x367_y311_w18_h9_seg2.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col12_x405_y312_w10_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col13_x437_y305_w8_h19.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row11_col14_x471_y316_w10_h4.png']]\n",
      "Lyrics List: [['Analysis\\\\bilawal_dhamaar\\\\0_row13_col1_x117_y343_w11_h14.png'], ['Analysis\\\\bilawal_dhamaar_segmented\\\\0_row13_col2_x137_y337_w20_h15_seg1.png', 'Analysis\\\\bilawal_dhamaar_segmented\\\\0_row13_col2_x137_y337_w20_h15_seg2.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col3_x165_y342_w15_h14.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col4_x190_y342_w8_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col5_x215_y342_w10_h11.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col6_x234_y342_w9_h12.png'], ['Analysis\\\\bilawal_dhamaar_segmented\\\\0_row13_col7_x256_y341_w13_h10_seg1.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col8_x282_y342_w15_h11.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col9_x311_y341_w7_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col10_x338_y342_w11_h11.png'], ['Analysis\\\\bilawal_dhamaar_segmented\\\\0_row13_col11_x367_y340_w15_h19_seg1.png', 'Analysis\\\\bilawal_dhamaar_segmented\\\\0_row13_col11_x367_y340_w15_h19_seg2.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col12_x405_y341_w9_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col13_x436_y341_w9_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row13_col14_x473_y340_w7_h12.png']]\n",
      "Swar Articulation Checks: [False, True, False, False, False, False, True, False, False, False, True, False, False, False]\n",
      "Lyrics Articulation Checks: [False, True, False, False, False, False, True, False, False, False, True, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (16, 19)\n",
      "Kann Swar List: [[], [], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row16_col5_x213_y374_w12_h14.png'], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row16_col9_x273_y376_w14_h12.png'], [], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row16_col14_x399_y377_w6_h10.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row16_col15_x438_y377_w9_h10.png'], []]\n",
      "Swar List: [['Analysis\\\\bilawal_dhamaar\\\\0_row18_col1_x116_y395_w15_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col2_x142_y400_w10_h4.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col3_x162_y400_w11_h4.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col4_x183_y395_w15_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col5_x215_y392_w14_h15.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col7_x239_y391_w15_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col8_x263_y399_w10_h5.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col10_x284_y395_w10_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col11_x308_y399_w9_h5.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col12_x338_y391_w13_h19.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col13_x372_y394_w9_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col14_x404_y394_w10_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col15_x437_y394_w10_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row18_col17_x468_y399_w12_h4.png']]\n",
      "Lyrics List: [['Analysis\\\\bilawal_dhamaar\\\\0_row19_col1_x115_y418_w15_h18.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col2_x141_y423_w7_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col3_x165_y423_w8_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col4_x183_y424_w11_h11.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col5_x214_y420_w9_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col7_x237_y423_w8_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col8_x263_y423_w7_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col10_x288_y424_w9_h14.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col11_x310_y423_w8_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col12_x338_y423_w7_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col13_x367_y424_w13_h11.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col14_x402_y417_w13_h22.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col15_x432_y423_w13_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col16_x463_y423_w8_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row19_col18_x478_y423_w5_h14.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (22, 24)\n",
      "Kann Swar List: [['Analysis\\\\bilawal_dhamaar\\\\0_row22_col1_x114_y477_w12_h14.png'], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row22_col6_x173_y478_w13_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row22_col7_x194_y481_w9_h10.png'], [], [], [], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row22_col14_x397_y477_w12_h13.png'], [], [], []]\n",
      "Swar List: [['Analysis\\\\bilawal_dhamaar\\\\0_row23_col1_x115_y495_w14_h15.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col2_x137_y502_w10_h5.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col4_x158_y502_w10_h5.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col6_x179_y494_w14_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col8_x204_y493_w14_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col9_x239_y494_w15_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col10_x275_y502_w9_h5.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col11_x306_y493_w15_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col12_x336_y503_w9_h4.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col13_x364_y494_w15_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col14_x403_y494_w15_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col15_x424_y493_w8_h18.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col16_x442_y493_w15_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row23_col17_x467_y502_w12_h4.png']]\n",
      "Lyrics List: [['Analysis\\\\bilawal_dhamaar\\\\0_row24_col1_x115_y527_w13_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col3_x143_y526_w8_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col5_x164_y526_w8_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col6_x185_y527_w10_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col8_x209_y527_w9_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col9_x238_y527_w14_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col10_x274_y526_w8_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col11_x304_y527_w12_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col12_x337_y527_w8_h11.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col13_x369_y527_w11_h15.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col14_x403_y522_w9_h21.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col15_x423_y526_w7_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col16_x449_y527_w10_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row24_col17_x471_y526_w8_h12.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (26, 27)\n",
      "Kann Swar List: [['Analysis\\\\bilawal_dhamaar\\\\0_row26_col1_x113_y560_w13_h13.png'], [], [], [], [], [], [], [], [], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col12_x397_y560_w12_h13.png'], [], [], []]\n",
      "Swar List: [['Analysis\\\\bilawal_dhamaar\\\\0_row26_col1_x114_y577_w15_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col2_x140_y576_w10_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col3_x161_y585_w9_h4.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col4_x179_y558_w8_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col5_x185_y576_w10_h18.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col6_x208_y585_w10_h4.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col7_x238_y585_w10_h5.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col8_x274_y577_w10_h16.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col9_x300_y561_w12_h31.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col10_x337_y585_w10_h4.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col11_x368_y585_w12_h4.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col12_x402_y576_w15_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col13_x424_y575_w7_h18.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col14_x441_y575_w15_h18.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row26_col15_x463_y576_w15_h17.png']]\n",
      "Lyrics List: [['Analysis\\\\bilawal_dhamaar\\\\0_row27_col1_x114_y604_w13_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col2_x142_y609_w8_h11.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col3_x160_y608_w8_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col5_x186_y610_w9_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col6_x210_y609_w8_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col7_x238_y609_w8_h11.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col8_x274_y609_w10_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col9_x304_y609_w9_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col10_x341_y609_w8_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col11_x372_y609_w7_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col12_x402_y603_w10_h18.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col13_x427_y609_w7_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col14_x446_y609_w9_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row27_col15_x470_y608_w8_h14.png']]\n",
      "Swar Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (29, 33)\n",
      "Kann Swar List: [['Analysis\\\\bilawal_dhamaar\\\\0_row29_col1_x112_y645_w10_h10.png'], [], [], [], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row29_col6_x233_y645_w10_h9.png'], [], ['Analysis\\\\bilawal_dhamaar\\\\0_row29_col8_x298_y646_w7_h10.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row29_col10_x332_y646_w9_h10.png'], [], [], [], [], []]\n",
      "Swar List: [['Analysis\\\\bilawal_dhamaar\\\\0_row30_col1_x113_y663_w11_h12.png'], ['Analysis\\\\bilawal_dhamaar_segmented\\\\0_row30_col2_x133_y658_w21_h12_seg1.png', 'Analysis\\\\bilawal_dhamaar_segmented\\\\0_row30_col2_x133_y658_w21_h12_seg2.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row30_col3_x163_y657_w8_h19.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row30_col4_x181_y658_w15_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row30_col5_x203_y658_w15_h17.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row30_col6_x239_y663_w10_h12.png'], ['Analysis\\\\bilawal_dhamaar_segmented\\\\0_row30_col7_x265_y658_w19_h14_seg1.png', 'Analysis\\\\bilawal_dhamaar_segmented\\\\0_row30_col7_x265_y658_w19_h14_seg2.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row30_col8_x303_y662_w10_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row30_col10_x336_y662_w11_h14.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row30_col11_x370_y667_w10_h4.png'], [], [], [], []]\n",
      "Lyrics List: [['Analysis\\\\bilawal_dhamaar\\\\0_row32_col1_x113_y688_w15_h16.png'], ['Analysis\\\\bilawal_dhamaar_segmented\\\\0_row32_col2_x138_y691_w13_h11_seg1.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row32_col3_x167_y692_w7_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row32_col4_x185_y692_w12_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row32_col5_x207_y692_w10_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row32_col6_x238_y691_w15_h13.png'], ['Analysis\\\\bilawal_dhamaar_segmented\\\\0_row32_col7_x270_y690_w13_h11_seg1.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row32_col8_x304_y691_w8_h12.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row32_col10_x334_y691_w14_h13.png'], ['Analysis\\\\bilawal_dhamaar\\\\0_row32_col11_x371_y691_w8_h12.png'], [], [], [], []]\n",
      "Swar Articulation Checks: [False, True, False, False, False, False, True, False, False, False, False, False, False, False]\n",
      "Lyrics Articulation Checks: [False, True, False, False, False, False, True, False, False, False, False, False, False, False]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.morphology import binary_erosion, binary_dilation, square\n",
    "from skimage import img_as_ubyte\n",
    "\n",
    "# Define the path to the folder to store segmented images\n",
    "segmented_folder_path = os.path.normpath('Analysis/bilawal_dhamaar_segmented')\n",
    "os.makedirs(segmented_folder_path, exist_ok=True)\n",
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess_image(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 21, 5)\n",
    "    return thresh\n",
    "\n",
    "# Function to separate articulation in an image\n",
    "def separate_articulation(image):\n",
    "    processed_image = preprocess_image(image)\n",
    "    contours, _ = cv2.findContours(processed_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        if 10 < h < 21 and w > 25:\n",
    "            upper_part = image[:y, :]\n",
    "            if upper_part.shape[0] > 0:\n",
    "                return upper_part, True  # Return the upper part and a flag indicating segmentation was successful\n",
    "            break\n",
    "    \n",
    "    return image, False  # Return the original image and a flag indicating no segmentation\n",
    "\n",
    "# Function to segment a word into multiple images\n",
    "def segment_image(img):\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian blur to reduce noise\n",
    "    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Apply simple binary thresholding and invert the image\n",
    "    _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Define structuring elements\n",
    "    structuring_element2 = np.ones((2, 2), dtype=bool)\n",
    "    structuring_element_erosion = square(3)\n",
    "\n",
    "    # Apply binary dilation to fill gaps\n",
    "    dilated = binary_dilation(binary, footprint=structuring_element2)\n",
    "\n",
    "    # Apply binary erosion to separate connected components\n",
    "    eroded = binary_erosion(dilated, footprint=structuring_element_erosion)\n",
    "    eroded = img_as_ubyte(eroded)  # Convert to uint8 for display purposes\n",
    "\n",
    "    # Perform vertical projection to find potential cut lines\n",
    "    vertical_projection = np.sum(eroded, axis=0)\n",
    "\n",
    "    # Find cut points by identifying valleys in the projection with heuristic\n",
    "    threshold = 0.15 * np.max(vertical_projection)\n",
    "    valleys = [x for x, y in enumerate(vertical_projection) if y < threshold]\n",
    "\n",
    "    # Apply heuristic: if two consecutive valleys are close, take the right one\n",
    "    cut_points = []\n",
    "    min_distance = 13\n",
    "    i = 0\n",
    "    while i < len(valleys) - 1:\n",
    "        if (valleys[i + 1] - valleys[i]) < min_distance:\n",
    "            cut_points.append(valleys[i + 1])\n",
    "            i += 2  # Skip the next valley since we took the right one\n",
    "        else:\n",
    "            cut_points.append(valleys[i])\n",
    "            i += 1\n",
    "    if i == len(valleys) - 1:\n",
    "        cut_points.append(valleys[i])  # Add the last valley if it's not processed\n",
    "\n",
    "    # Ensure no duplicate cut points and sort them\n",
    "    cut_points = sorted(set(cut_points))\n",
    "\n",
    "    # Separate the image at cut points\n",
    "    cut_images = []\n",
    "    start = 0\n",
    "    for cut_point in cut_points:\n",
    "        if cut_point - start > 10:  # Ensure segments are large enough\n",
    "            cut_image = img[:, start:cut_point]\n",
    "            cut_images.append(cut_image)\n",
    "            start = cut_point\n",
    "\n",
    "    # Add the last segment\n",
    "    cut_images.append(img[:, start:])\n",
    "\n",
    "    return cut_images\n",
    "\n",
    "# Function to merge segments based on height-to-width ratio\n",
    "def merge_segments(segments):\n",
    "    final_images = []\n",
    "    i = 0\n",
    "    while i < len(segments):\n",
    "        current_image = segments[i]\n",
    "        current_ratio = current_image.shape[0] / current_image.shape[1]\n",
    "\n",
    "        ratio_threshold = 1.8\n",
    "\n",
    "        if current_image.shape[0] > 35:\n",
    "            ratio_threshold = 2.9\n",
    "        \n",
    "        # If the ratio is greater than the threshold and it's the first segment\n",
    "        if current_ratio > ratio_threshold and i == 0:\n",
    "            # Merge with the next segment\n",
    "            if i + 1 < len(segments):\n",
    "                current_image = np.hstack((current_image, segments[i + 1]))\n",
    "                final_images.append(current_image)\n",
    "                i += 2\n",
    "            else:\n",
    "                final_images.append(current_image)\n",
    "                i += 1\n",
    "        # If two or more consecutive segments have a ratio greater than the threshold\n",
    "        elif i < len(segments) - 1 and (segments[i + 1].shape[0] / segments[i + 1].shape[1]) > ratio_threshold:\n",
    "            while i < len(segments) - 1 and (segments[i + 1].shape[0] / segments[i + 1].shape[1]) > ratio_threshold:\n",
    "                current_image = np.hstack((current_image, segments[i + 1]))\n",
    "                i += 1\n",
    "            final_images.append(current_image)\n",
    "            i += 1\n",
    "        # If the ratio is greater than the threshold and it's not the first segment\n",
    "        elif current_ratio > ratio_threshold and i != 0:\n",
    "            # Merge with the previous segment\n",
    "            if final_images:\n",
    "                final_images[-1] = np.hstack((final_images[-1], current_image))\n",
    "            else:\n",
    "                final_images.append(current_image)\n",
    "            i += 1\n",
    "        else:\n",
    "            final_images.append(current_image)\n",
    "            i += 1\n",
    "\n",
    "    return final_images\n",
    "\n",
    "# Function to process a single image, segment, and save the results in the provided folder\n",
    "def segment_word(image_path, output_folder):\n",
    "    # Load the image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        return []\n",
    "    \n",
    "    # Segment the image\n",
    "    segmented_images = segment_image(img)\n",
    "    \n",
    "    # Merge segments based on height-to-width ratio\n",
    "    final_images = merge_segments(segmented_images)\n",
    "    \n",
    "    # Save the segmented images\n",
    "    image_base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    segmented_paths = []\n",
    "    for i, segmented_image in enumerate(final_images):\n",
    "        seg_image_path = os.path.normpath(os.path.join(output_folder, f'{image_base_name}_seg{i+1}.png'))\n",
    "        cv2.imwrite(seg_image_path, segmented_image)\n",
    "        segmented_paths.append(seg_image_path)\n",
    "    \n",
    "    return segmented_paths\n",
    "\n",
    "# Function to update lists based on segmentation\n",
    "def update_lists_with_segmentation(subgroup_results):\n",
    "    for subgroup_range, results in subgroup_results.items():\n",
    "        swar_list = results['swar_list']\n",
    "        lyrics_list = results['lyrics_list']\n",
    "        swar_articulation_checks = results['swar_articulation_checks']\n",
    "        lyrics_articulation_checks = results['lyrics_articulation_checks']\n",
    "        \n",
    "        # Apply articulation segmentation to swar row\n",
    "        for i in range(len(swar_list)):\n",
    "            if not swar_articulation_checks[i] and swar_list[i]:  # Check if articulation is False and the list is not empty\n",
    "                swar_image_path = swar_list[i][0]  # Get the image path\n",
    "                swar_image = cv2.imread(swar_image_path)  # Load the image\n",
    "                if swar_image is not None:\n",
    "                    segmented_image, is_segmented = separate_articulation(swar_image)\n",
    "                    if is_segmented:\n",
    "                        swar_articulation_checks[i] = True  # Update articulation check\n",
    "                        # Save the segmented image with the original name\n",
    "                        original_name = os.path.basename(swar_image_path)\n",
    "                        seg_image_path = os.path.normpath(os.path.join(segmented_folder_path, original_name))\n",
    "                        cv2.imwrite(seg_image_path, segmented_image)\n",
    "                        swar_list[i] = [seg_image_path]  # Update the list with the new image path\n",
    "        \n",
    "        # Apply articulation segmentation to lyrics row\n",
    "        for i in range(len(lyrics_list)):\n",
    "            if not lyrics_articulation_checks[i] and lyrics_list[i]:  # Check if articulation is False and the list is not empty\n",
    "                lyrics_image_path = lyrics_list[i][0]  # Get the image path\n",
    "                lyrics_image = cv2.imread(lyrics_image_path)  # Load the image\n",
    "                if lyrics_image is not None:\n",
    "                    segmented_image, is_segmented = separate_articulation(lyrics_image)\n",
    "                    if is_segmented:\n",
    "                        lyrics_articulation_checks[i] = True  # Update articulation check\n",
    "                        # Save the segmented image with the original name\n",
    "                        original_name = os.path.basename(lyrics_image_path)\n",
    "                        seg_image_path = os.path.normpath(os.path.join(segmented_folder_path, original_name))\n",
    "                        cv2.imwrite(seg_image_path, segmented_image)\n",
    "                        lyrics_list[i] = [seg_image_path]  # Update the list with the new image path\n",
    "        \n",
    "        # Apply word segmentation to swar row\n",
    "        for i in range(len(swar_list)):\n",
    "            if swar_articulation_checks[i] and swar_list[i]:  # Check if articulation is True and the list is not empty\n",
    "                swar_image_path = swar_list[i][0]  # Get the image path\n",
    "                segmented_paths = segment_word(swar_image_path, segmented_folder_path)\n",
    "                if segmented_paths:\n",
    "                    swar_list[i] = segmented_paths  # Update the list with segmented image paths\n",
    "        \n",
    "        # Apply word segmentation to lyrics row\n",
    "        for i in range(len(lyrics_list)):\n",
    "            if lyrics_articulation_checks[i] and lyrics_list[i]:  # Check if articulation is True and the list is not empty\n",
    "                lyrics_image_path = lyrics_list[i][0]  # Get the image path\n",
    "                segmented_paths = segment_word(lyrics_image_path, segmented_folder_path)\n",
    "                if segmented_paths:\n",
    "                    lyrics_list[i] = segmented_paths  # Update the list with segmented image paths\n",
    "        \n",
    "        # Update the results\n",
    "        subgroup_results[subgroup_range]['swar_list'] = swar_list\n",
    "        subgroup_results[subgroup_range]['lyrics_list'] = lyrics_list\n",
    "        subgroup_results[subgroup_range]['swar_articulation_checks'] = swar_articulation_checks\n",
    "        subgroup_results[subgroup_range]['lyrics_articulation_checks'] = lyrics_articulation_checks\n",
    "\n",
    "# Example usage\n",
    "update_lists_with_segmentation(subgroup_results)\n",
    "\n",
    "# Print the updated results for each subgroup\n",
    "for subgroup_range, results in subgroup_results.items():\n",
    "    print(f\"Subgroup Range: {subgroup_range}\")\n",
    "    print(f\"Kann Swar List: {results['kann_swar_list']}\")\n",
    "    print(f\"Swar List: {results['swar_list']}\")\n",
    "    print(f\"Lyrics List: {results['lyrics_list']}\")\n",
    "    print(f\"Swar Articulation Checks: {results['swar_articulation_checks']}\")\n",
    "    print(f\"Lyrics Articulation Checks: {results['lyrics_articulation_checks']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Subgroup Range: (4, 7)\n",
      "Predicted Swar List: [[], [], [], [], [], [], [], [], [], [], [], ['ध'], ['ग'], ['-']]\n",
      "Predicted Kann Swar List: [[], [], [], [], [], [], [], [], [], [], [], ['प'], ['प'], []]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (7, 8)\n",
      "Predicted Swar List: [['प'], ['-'], ['नि'], ['नि'], ['सा॑'], ['-'], ['-'], ['नि'], ['नि'], ['सा॑'], ['-'], ['रे॑'], ['सा॑'], ['-']]\n",
      "Predicted Kann Swar List: [[], [], ['सा॑'], [], [], [], [], [], ['ध'], [], [], ['सा॑'], [], []]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (10, 14)\n",
      "Predicted Swar List: [['ध'], ['ध़', 'नि॒'], ['प'], ['-'], ['ग'], ['ग'], ['म॑', 'रे'], ['ग'], ['म'], ['प'], ['सा', '|'], ['ग'], ['रे'], ['-']]\n",
      "Predicted Kann Swar List: [['सा॑'], [], [], [], ['प'], ['प'], [], ['म'], [], [], [], [], [], []]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (16, 19)\n",
      "Predicted Swar List: [['सा'], ['-'], ['-'], ['सा'], ['सा'], ['सा॑'], ['-'], ['ध'], ['-'], ['नि॒'], ['प'], ['ध'], ['ग'], ['-']]\n",
      "Predicted Kann Swar List: [[], [], [], [], ['नि'], [], [], ['सा'], [], [], [], ['प'], ['ध'], []]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (22, 24)\n",
      "Predicted Swar List: [['सा'], ['-'], ['-'], ['नि'], ['नि'], ['सा॑'], ['-'], ['सा॑'], ['-'], ['सा॑'], ['सा॑'], ['रे॑'], ['सा॑'], ['-']]\n",
      "Predicted Kann Swar List: [['नि'], [], [], ['सा॑'], ['ध'], [], [], [], [], [], ['नि'], [], [], []]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (26, 27)\n",
      "Predicted Swar List: [['सा॑'], ['ग॑'], ['-'], ['रे॑'], ['ग॑'], ['-'], ['-'], [\"म॑'\"], ['प़'], ['-'], ['-'], ['सा॑'], ['रे॑'], ['सा॑'], ['सा॑']]\n",
      "Predicted Kann Swar List: [['नि'], [], [], [], [], [], [], [], [], [], [], ['नि'], [], [], []]\n",
      "--------------------------------------------------------------------------------\n",
      "Subgroup Range: (29, 33)\n",
      "Predicted Swar List: [['ध'], ['सा॑', 'नि'], ['रे॑'], ['सा॑'], ['सा॑'], ['ध'], ['नि॒', 'प॑'], ['ध'], ['ग'], ['-'], [], [], [], []]\n",
      "Predicted Kann Swar List: [['सा'], [], [], [], [], ['सा'], [], ['प'], ['ध'], [], [], [], [], []]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('cnn_recognizer_music_15_v1.h5')\n",
    "\n",
    "# Preprocess the input image\n",
    "def preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Unable to read image at path: {image_path}\")\n",
    "    image = cv2.resize(image, (32, 32))  # Resize to match the model's input size\n",
    "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "# Pass the image through the model and get predictions\n",
    "def predict_class(image_path):\n",
    "    preprocessed_image = preprocess_image(image_path)\n",
    "    predictions = model.predict(preprocessed_image)\n",
    "    predicted_class_index = np.argmax(predictions, axis=1)\n",
    "    max_probability = np.max(predictions, axis=1)\n",
    "    return predicted_class_index[0], max_probability[0]\n",
    "\n",
    "# Define the classes\n",
    "# classes = [\"सा\", \"रे\", \"ग\", \"म\", \"प\", \"ध\", \"नि\", \"रे॒\", \"ग॒\", \"ध॒\", \"नि॒\", \"म॑\", \n",
    "#            \"सा\\u0951\", \"रे\\u0951\", \"ग\\u0951\", \"म\\u0951\", \"प\\u0951\", \"ध\\u0951\", \"रे॒\\u0951\", \"ग॒\\u0951\", \"म॑'\", \n",
    "#            \"म\\u093C\", \"म॑\\u093C\", \"प\\u093C\", \"ध॒\\u093C\", \"ध\\u093C\", \"नि॒\\u093C\", \"नि\\u093C\", \n",
    "#            \")\", \",\", \"-\", \"४\", \"O\", \"(\", \"^^\", \"X\", \"३\", \"२\", \"|\", \"<_>\"]\n",
    "\n",
    "classes = [\"saa\", \"re\", \"ga\", \"ma\", \"pa\", \"dha\", \"ni\", \"re-\", \"ga-\", \"dha-\", \"ni-\", \"ma#\", \n",
    "           \"saa'\", \"re'\", \"ga'\", \"ma'\", \"pa'\", \"dha'\", \"re-'\", \"ga-'\", \"ma#'\", \n",
    "           \"ma,\", \"ma#,\", \"pa,\", \"dha-,\", \"dha,\", \"ni-,\", \"ni,\", \n",
    "           \")\", \",\", \"-\", \"४\", \"O\", \"(\", \"^^\", \"X\", \"३\", \"२\", \"|\", \"<_>\"]\n",
    "\n",
    "# classes = [\"c\", \"d\", \"e\", \"f\", \"g\", \"a\", \"b\", \"d-\", \"e-\", \"a-\", \"b-\", \"f#\", \n",
    "#            \"cc\", \"dd\", \"ee\", \"ff\", \"gg\", \"aa\", \"dd-\", \"ee-\", \"ff#\", \n",
    "#            \"F\", \"F#\", \"G\", \"A-\", \"A\", \"B-\", \"B\", \n",
    "#            \")\", \",\", \"-\", \"४\", \"O\", \"(\", \"^^\", \"X\", \"३\", \"२\", \"|\", \"<_>\"]\n",
    "\n",
    "# Function to generate new lists with predicted class names\n",
    "def generate_predicted_lists(subgroup_results):\n",
    "    predicted_results = {}\n",
    "    \n",
    "    for subgroup_range, results in subgroup_results.items():\n",
    "        # Initialize new lists for predicted class names\n",
    "        predicted_swar_list = []\n",
    "        predicted_kann_swar_list = []  # Add this if you have kann_swar_list\n",
    "        \n",
    "        # Predict class names for swar_list\n",
    "        for image_paths in results['swar_list']:\n",
    "            if image_paths:  # Check if the list is not empty\n",
    "                predicted_classes = []\n",
    "                for image_path in image_paths:\n",
    "                    predicted_class_index, _ = predict_class(image_path)\n",
    "                    predicted_class_name = classes[predicted_class_index]\n",
    "                    predicted_classes.append(predicted_class_name)\n",
    "                predicted_swar_list.append(predicted_classes)\n",
    "            else:\n",
    "                predicted_swar_list.append([])  # Append empty list for empty entries\n",
    "        \n",
    "        # Predict class names for kann_swar_list (if applicable)\n",
    "        for image_paths in results['kann_swar_list']:\n",
    "            if image_paths:  # Check if the list is not empty\n",
    "                predicted_classes = []\n",
    "                for image_path in image_paths:\n",
    "                    predicted_class_index, _ = predict_class(image_path)\n",
    "                    predicted_class_name = classes[predicted_class_index]\n",
    "                    predicted_classes.append(predicted_class_name)\n",
    "                predicted_kann_swar_list.append(predicted_classes)\n",
    "            else:\n",
    "                predicted_kann_swar_list.append([])  # Append empty list for empty entries\n",
    "        \n",
    "        # Store the predicted results for this subgroup\n",
    "        predicted_results[subgroup_range] = {\n",
    "            'predicted_swar_list': predicted_swar_list,\n",
    "            'predicted_kann_swar_list': predicted_kann_swar_list  \n",
    "        }\n",
    "    \n",
    "    return predicted_results\n",
    "\n",
    "# Example usage\n",
    "# Assuming subgroup_results is the dictionary you provided\n",
    "predicted_results = generate_predicted_lists(subgroup_results)\n",
    "\n",
    "# Print the predicted results\n",
    "for subgroup_range, results in predicted_results.items():\n",
    "    print(f\"Subgroup Range: {subgroup_range}\")\n",
    "    print(f\"Predicted Swar List: {results['predicted_swar_list']}\")\n",
    "    print(f\"Predicted Kann Swar List: {results['predicted_kann_swar_list']}\") \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store taal information\n",
    "taal_info = {\n",
    "    \"Rupak\": {\n",
    "        \"beat_count\": 7,\n",
    "        \"divisions\": [3, 2, 2],\n",
    "        \"vibhaag\": [\"X\", \"2\", \"3\"],\n",
    "        \"time_signature\": \"7/8\"  # 7 beats, divided into 3+2+2\n",
    "    },\n",
    "    \"Sultaal\": {\n",
    "        \"beat_count\": 10,\n",
    "        \"divisions\": [2, 2, 2, 2, 2],\n",
    "        \"vibhaag\": [\"X\", \"0\", \"2\", \"3\", \"0\"],\n",
    "        \"time_signature\": \"10/8\"  # 10 beats, divided into 2+2+2+2+2\n",
    "    },\n",
    "    \"Chautaal\": {\n",
    "        \"beat_count\": 12,\n",
    "        \"divisions\": [2, 2, 2, 2, 2, 2],\n",
    "        \"vibhaag\": [\"X\", \"0\", \"2\", \"0\", \"3\", \"4\"],\n",
    "        \"time_signature\": \"12/8\"  # 12 beats, divided into 2+2+2+2+2+2\n",
    "    },\n",
    "    \"Ada Chautaal\": {\n",
    "        \"beat_count\": 14,\n",
    "        \"divisions\": [2, 2, 2, 2, 2, 2, 2],\n",
    "        \"vibhaag\": [\"X\", \"2\", \"0\", \"3\", \"0\", \"4\", \"0\"],\n",
    "        \"time_signature\": \"14/8\"  # 14 beats, divided into 2+2+2+2+2+2+2\n",
    "    },\n",
    "    \"Jhoomra\": {\n",
    "        \"beat_count\": 14,\n",
    "        \"divisions\": [3, 4, 3, 4],\n",
    "        \"vibhaag\": [\"X\", \"2\", \"0\", \"3\"],\n",
    "        \"time_signature\": \"14/8\"  # 14 beats, divided into 3+4+3+4\n",
    "    },\n",
    "    \"Dhamaar\": {\n",
    "        \"beat_count\": 14,\n",
    "        \"divisions\": [5, 2, 3, 4],\n",
    "        \"vibhaag\": [\"X\", \"2\", \"0\", \"3\"],\n",
    "        \"time_signature\": \"14/8\"  # 14 beats, divided into 5+2+3+4\n",
    "    },\n",
    "    \"Deepchandi\": {\n",
    "        \"beat_count\": 14,\n",
    "        \"divisions\": [3, 4, 3, 4],\n",
    "        \"vibhaag\": [\"X\", \"2\", \"0\", \"3\"],\n",
    "        \"time_signature\": \"14/8\"  # 14 beats, divided into 3+4+3+4\n",
    "    },\n",
    "    \"Punjabi (Tilwada)\": {\n",
    "        \"beat_count\": 16,\n",
    "        \"divisions\": [4, 4, 4, 4],\n",
    "        \"vibhaag\": [\"X\", \"2\", \"0\", \"3\"],\n",
    "        \"time_signature\": \"16/8\"  # 16 beats, divided into 4+4+4+4\n",
    "    },\n",
    "    \"Dadra\": {\n",
    "        \"beat_count\": 6,\n",
    "        \"divisions\": [3, 3],\n",
    "        \"vibhaag\": [\"X\", \"0\"],\n",
    "        \"time_signature\": \"6/8\"  # 6 beats, divided into 3+3\n",
    "    },\n",
    "    \"Jhaptal\": {\n",
    "        \"beat_count\": 10,\n",
    "        \"divisions\": [2, 3, 2, 3],\n",
    "        \"vibhaag\": [\"X\", \"2\", \"0\", \"3\"],\n",
    "        \"time_signature\": \"10/8\"  # 10 beats, divided into 2+3+2+3\n",
    "    },\n",
    "    \"Ektaal\": {\n",
    "        \"beat_count\": 12,\n",
    "        \"divisions\": [2, 2, 2, 2, 2, 2],\n",
    "        \"vibhaag\": [\"X\", \"0\", \"2\", \"0\", \"3\", \"4\"],\n",
    "        \"time_signature\": \"12/8\"  # 12 beats, divided into 2+2+2+2+2+2\n",
    "    },\n",
    "    \"Teentaal\": {\n",
    "        \"beat_count\": 16,\n",
    "        \"divisions\": [4, 4, 4, 4],\n",
    "        \"vibhaag\": [\"X\", \"2\", \"0\", \"3\"],\n",
    "        \"time_signature\": \"16/8\"  # 16 beats, divided into 4+4+4+4\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Sam positions for Dhamaar: [1, 5, 8, 10]\n",
      "\n",
      "Taal: Dhamaar\n",
      "Sam (X) at beat: 5\n",
      "New Divisions: [4, 5, 2, 3]\n",
      "New Vibhaag: ['3', 'X', '2', '0']\n"
     ]
    }
   ],
   "source": [
    "def calculate_valid_sam_positions(taal_name):\n",
    "    \"\"\"\n",
    "    Calculate all valid positions where the sam (X) can occur for a given taal.\n",
    "    \"\"\"\n",
    "    if taal_name not in taal_info:\n",
    "        raise ValueError(f\"Taal '{taal_name}' not found in the database.\")\n",
    "    \n",
    "    # Get the divisions for the taal\n",
    "    divisions = taal_info[taal_name][\"divisions\"]\n",
    "    \n",
    "    # Calculate valid sam positions\n",
    "    valid_positions = [1]  # The first beat is always valid\n",
    "    cumulative_sum = 0\n",
    "    \n",
    "    # Start from the end of the divisions and add cumulatively\n",
    "    for i in range(len(divisions) - 1, -1, -1):\n",
    "        cumulative_sum += divisions[i]\n",
    "        if cumulative_sum < taal_info[taal_name][\"beat_count\"]:\n",
    "            valid_positions.append(cumulative_sum + 1)\n",
    "    \n",
    "    return sorted(valid_positions)\n",
    "\n",
    "def get_user_input():\n",
    "    \"\"\"\n",
    "    Take input from the user: taal name and beat number for sam.\n",
    "    \"\"\"\n",
    "    taal_name = input(\"Enter the Taal name: \")\n",
    "    if taal_name not in taal_info:\n",
    "        raise ValueError(f\"Taal '{taal_name}' not found in the database.\")\n",
    "    \n",
    "    valid_positions = calculate_valid_sam_positions(taal_name)\n",
    "    print(f\"Valid Sam positions for {taal_name}: {valid_positions}\")\n",
    "    \n",
    "    sam_beat = int(input(f\"Enter the beat number for Sam (X) (valid positions: {valid_positions}): \"))\n",
    "    if sam_beat not in valid_positions:\n",
    "        raise ValueError(f\"Invalid Sam position. Valid positions are: {valid_positions}\")\n",
    "    \n",
    "    return taal_name, sam_beat\n",
    "\n",
    "def calculate_divisions_and_vibhaag(taal_name, sam_beat):\n",
    "    \"\"\"\n",
    "    Calculate the divisions and vibhaag for the given taal and sam beat.\n",
    "    \"\"\"\n",
    "    if taal_name not in taal_info:\n",
    "        raise ValueError(f\"Taal '{taal_name}' not found in the database.\")\n",
    "    \n",
    "    # Get the divisions and vibhaag for the taal\n",
    "    divisions = taal_info[taal_name][\"divisions\"]\n",
    "    vibhaag = taal_info[taal_name][\"vibhaag\"]\n",
    "    \n",
    "    # If sam is at the first beat, no change is needed\n",
    "    if sam_beat == 1:\n",
    "        return divisions, vibhaag\n",
    "    \n",
    "    # Calculate the rotation index\n",
    "    cumulative_sum = 0\n",
    "    rotation_index = 0\n",
    "    \n",
    "    # Start from the end of the divisions and add cumulatively\n",
    "    for i in range(len(divisions) - 1, -1, -1):\n",
    "        cumulative_sum += divisions[i]\n",
    "        if cumulative_sum + 1 == sam_beat:\n",
    "            rotation_index = i\n",
    "            break\n",
    "    \n",
    "    # Rotate divisions and vibhaag\n",
    "    new_divisions = divisions[rotation_index:] + divisions[:rotation_index]\n",
    "    new_vibhaag = vibhaag[rotation_index:] + vibhaag[:rotation_index]\n",
    "    \n",
    "    return new_divisions, new_vibhaag\n",
    "\n",
    "\n",
    "# Step 1: Get user input\n",
    "taal_name, sam_beat = get_user_input()\n",
    "\n",
    "# Step 2: Calculate divisions and vibhaag\n",
    "new_divisions, new_vibhaag = calculate_divisions_and_vibhaag(taal_name, sam_beat)\n",
    "\n",
    "# Step 3: Print the results\n",
    "print(f\"\\nTaal: {taal_name}\")\n",
    "print(f\"Sam (X) at beat: {sam_beat}\")\n",
    "print(f\"New Divisions: {new_divisions}\")\n",
    "print(f\"New Vibhaag: {new_vibhaag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Kern Code:\n",
      "\n",
      "!!!raag: Yaman Kalyan\n",
      "!!!taal: Ektaal\n",
      "!!!lay: drut\n",
      "**kern\n",
      "*M12/8\n",
      "*MM150\n",
      "*Isitar\n",
      "*c:\n",
      "!! Sthayee or Antara\n",
      "=\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to generate kern code based on user input\n",
    "def generate_kern_code(raag, taal, lay):\n",
    "    # Check if the taal exists in the dictionary\n",
    "    if taal in taal_info:\n",
    "        time_signature = taal_info[taal][\"time_signature\"]\n",
    "    else:\n",
    "        time_signature = \"4/4\"  # Default to 4/4 if taal is not recognized\n",
    "\n",
    "    # Set tempo based on lay\n",
    "    if lay.lower() == \"vilambit\":\n",
    "        tempo = 60  # Slow tempo\n",
    "    elif lay.lower() == \"madhya\":\n",
    "        tempo = 90  # Medium tempo\n",
    "    elif lay.lower() == \"drut\":\n",
    "        tempo = 150  # Fast tempo\n",
    "    else:\n",
    "        tempo = 60  # Default to Vilambit if lay is not recognized\n",
    "\n",
    "    # Generate the kern code\n",
    "    kern_code = f\"\"\"!!!raag: {raag}\n",
    "!!!taal: {taal}\n",
    "!!!lay: {lay}\n",
    "**kern\n",
    "*M{time_signature}\n",
    "*MM{tempo}\n",
    "*Isitar\n",
    "*c:\n",
    "!! Sthayee or Antara\n",
    "=\n",
    "\"\"\"\n",
    "    return kern_code\n",
    "\n",
    "# Take user input\n",
    "raag = input(\"Enter the Raag (e.g., Yaman): \")\n",
    "taal = input(\"Enter the Taal (e.g., Teentaal): \")\n",
    "lay = input(\"Enter the Lay (Tempo) (e.g., Vilambit): \")\n",
    "\n",
    "# Generate and display the kern code\n",
    "kern_output = generate_kern_code(raag, taal, lay)\n",
    "print(\"Generated Kern Code:\\n\")\n",
    "print(kern_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!Subgroup Range: (4, 7)\n",
      "==1\n",
      "4ryy\n",
      "4ryy\n",
      "4ryy\n",
      "4ryy\n",
      "=\n",
      "4ryy\n",
      "4ryy\n",
      "4ryy\n",
      "4ryy\n",
      "4ryy\n",
      "=\n",
      "4ryy\n",
      "4ryy\n",
      "=\n",
      "gq\n",
      "4a\n",
      "aq\n",
      "2e\n",
      "!!!Subgroup Range: (7, 8)\n",
      "==2\n",
      "2g\n",
      "ccq\n",
      "4b\n",
      "aq\n",
      "4b\n",
      "=\n",
      "2.cc\n",
      "4b\n",
      "aq\n",
      "4b\n",
      "=\n",
      "2cc\n",
      "=\n",
      "ccq\n",
      "4dd\n",
      "2cc\n",
      "!!!Subgroup Range: (10, 14)\n",
      "==3\n",
      "ccq\n",
      "4a\n",
      "8a\n",
      "8b-\n",
      "2g\n",
      "=\n",
      "gq\n",
      "4e\n",
      "gq\n",
      "4e\n",
      "8f\n",
      "8d\n",
      "fq\n",
      "4e\n",
      "4f\n",
      "=\n",
      "4g\n",
      "8f\n",
      "8e\n",
      "=\n",
      "4e\n",
      "2d\n",
      "!!!Subgroup Range: (16, 19)\n",
      "==4\n",
      "2.c\n",
      "4c\n",
      "=\n",
      "Bq\n",
      "4cc\n",
      "2cc\n",
      "ccq\n",
      "2a\n",
      "=\n",
      "4b-\n",
      "4g\n",
      "=\n",
      "gq\n",
      "4a\n",
      "aq\n",
      "2e\n"
     ]
    }
   ],
   "source": [
    "# Mapping of Indian swar to kern symbols\n",
    "swar_to_kern = {\n",
    "    \"saa\": \"c\", \"re\": \"d\", \"ga\": \"e\", \"ma\": \"f\", \"pa\": \"g\", \"dha\": \"a\", \"ni\": \"b\",\n",
    "    \"re-\": \"d-\", \"ga-\": \"e-\", \"dha-\": \"a-\", \"ni-\": \"b-\", \"ma#\": \"f#\",\n",
    "    \"saa'\": \"cc\", \"re'\": \"dd\", \"ga'\": \"ee\", \"ma'\": \"ff\", \"pa'\": \"gg\", \"dha'\": \"aa\", \"ni'\": \"bb\",\n",
    "    \"re-'\": \"dd-\", \"ga-'\": \"ee-\", \"ma#'\": \"ff#\",\n",
    "    \"ma,\": \"F\", \"ma#,\": \"F#\", \"pa,\": \"G\", \"dha-,\": \"A-\", \"dha,\": \"A\", \"ni-,\": \"B-\", \"ni,\": \"B\",\n",
    "    \")\": \")\", \",\": \",\", \"-\": \"-\", \"४\": \"४\", \"O\": \"O\", \"(\": \"(\", \"^^\": \"^^\", \"X\": \"X\", \"३\": \"३\", \"२\": \"२\", \"|\": \"|\", \"<_>\": \"<_>\"\n",
    "}\n",
    "\n",
    "# Function to convert swar list to kern code\n",
    "def convert_to_kern(predicted_swar_list, predicted_kann_swar_list, divisions, subgroup_number):\n",
    "    kern_code = [f\"=={subgroup_number}\"]  # Start with subgroup number\n",
    "    current_division_index = 0\n",
    "    division_length = divisions[current_division_index]\n",
    "    division_counter = 0\n",
    "\n",
    "    i = 0\n",
    "    while i < len(predicted_swar_list):\n",
    "        # Check if we need to add a barline\n",
    "        if division_counter >= division_length:\n",
    "            kern_code.append(\"=\")\n",
    "            current_division_index += 1\n",
    "            if current_division_index < len(divisions):\n",
    "                division_length = divisions[current_division_index]\n",
    "            division_counter = 0\n",
    "\n",
    "        # Handle Kann Swar\n",
    "        if predicted_kann_swar_list[i]:\n",
    "            for kann_swar in predicted_kann_swar_list[i]:\n",
    "                if kann_swar in swar_to_kern:\n",
    "                    kern_code.append(f\"{swar_to_kern[kann_swar]}q\")\n",
    "\n",
    "        # Handle Swar\n",
    "        if predicted_swar_list[i]:\n",
    "            if predicted_swar_list[i] == [\"-\"]:\n",
    "                # Handle one extension\n",
    "                if i + 1 < len(predicted_swar_list) and predicted_swar_list[i + 1] == [\"-\"]:\n",
    "                    # Handle two extensions\n",
    "                    if kern_code and not kern_code[-1].startswith(\"=\"):\n",
    "                        last_note = kern_code[-1]\n",
    "                        kern_code[-1] = f\"2.{last_note[1:]}\"\n",
    "                    i += 1  # Skip the next '-'\n",
    "                    division_counter += 1\n",
    "                else:\n",
    "                    # Handle one extension\n",
    "                    if kern_code and not kern_code[-1].startswith(\"=\"):\n",
    "                        last_note = kern_code[-1]\n",
    "                        kern_code[-1] = f\"2{last_note[1:]}\"\n",
    "            else:\n",
    "                # Handle multiple swars in the list\n",
    "                num_swars = len(predicted_swar_list[i])\n",
    "                duration = 4 * num_swars\n",
    "                for swar in predicted_swar_list[i]:\n",
    "                    if swar in swar_to_kern:\n",
    "                        kern_code.append(f\"{duration}{swar_to_kern[swar]}\")\n",
    "        else:\n",
    "            # Handle empty list (rest)\n",
    "            kern_code.append(\"4ryy\")\n",
    "\n",
    "        division_counter += 1\n",
    "        i += 1\n",
    "\n",
    "    return kern_code\n",
    "\n",
    "# Hardcoded predicted results\n",
    "predicted_results = {\n",
    "    (4, 7): {\n",
    "        \"predicted_swar_list\": [[], [], [], [], [], [], [], [], [], [], [], ['dha'], ['ga'], ['-']],\n",
    "        \"predicted_kann_swar_list\": [[], [], [], [], [], [], [], [], [], [], [], ['pa'], ['dha'], []]\n",
    "    },\n",
    "    (7, 8): {\n",
    "        \"predicted_swar_list\": [['pa'], ['-'], ['ni'], ['ni'], [\"saa'\"], ['-'], ['-'], ['ni'], ['ni'], [\"saa'\"], ['-'], [\"re'\"], [\"saa'\"], ['-']],\n",
    "        \"predicted_kann_swar_list\": [[], [], [\"saa'\"], ['dha'], [], [], [], [], ['dha'], [], [], [\"saa'\"], [], []]\n",
    "    },\n",
    "    (10, 14): {\n",
    "        \"predicted_swar_list\": [['dha'], ['dha', 'ni-'], ['pa'], ['-'], ['ga'], ['ga'], ['ma', 're'], ['ga'], ['ma'], ['pa'], ['ma', 'ga'], ['ga'], ['re'], ['-']],\n",
    "        \"predicted_kann_swar_list\": [[\"saa'\"], [], [], [], ['pa'], ['pa'], [], ['ma'], [], [], [], [], [], []]\n",
    "    },\n",
    "    (16, 19): {\n",
    "        \"predicted_swar_list\": [['saa'], ['-'], ['-'], ['saa'], [\"saa'\"], [\"saa'\"], ['-'], ['dha'], ['-'], ['ni-'], ['pa'], ['dha'], ['ga'], ['-']],\n",
    "        \"predicted_kann_swar_list\": [[], [], [], [], ['ni,'], [], [], [\"saa'\"], [], [], [], ['pa'], ['dha'], []]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Divisions for the example\n",
    "divisions = [4, 5, 2, 3]\n",
    "\n",
    "# Generate kern code for each subgroup\n",
    "for subgroup_number, (subgroup_range, results) in enumerate(predicted_results.items(), start=1):\n",
    "    print(f\"!!!Subgroup Range: {subgroup_range}\")\n",
    "    kern_code = convert_to_kern(results[\"predicted_swar_list\"], results[\"predicted_kann_swar_list\"], divisions, subgroup_number)\n",
    "    print(\"\\n\".join(kern_code))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
